{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x (100, 1)\n",
      "Train Epoch: 0 \tLoss eqn: 0.0016077448 Loss data: 0.6940560341\n",
      "learning rate is  0.001\n",
      "Train Epoch: 10 \tLoss eqn: 0.0389310494 Loss data: 0.0162570495\n",
      "learning rate is  0.001\n",
      "Train Epoch: 20 \tLoss eqn: 0.0644852743 Loss data: 0.0109021720\n",
      "learning rate is  0.001\n",
      "Train Epoch: 30 \tLoss eqn: 0.0389151201 Loss data: 0.0122379716\n",
      "learning rate is  0.001\n",
      "Train Epoch: 40 \tLoss eqn: 0.0550070070 Loss data: 0.0099612009\n",
      "learning rate is  0.001\n",
      "Train Epoch: 50 \tLoss eqn: 0.0553533435 Loss data: 0.0097460952\n",
      "learning rate is  0.001\n",
      "Train Epoch: 60 \tLoss eqn: 0.0471335202 Loss data: 0.0105041219\n",
      "learning rate is  0.001\n",
      "Train Epoch: 70 \tLoss eqn: 0.0544231646 Loss data: 0.0097527718\n",
      "learning rate is  0.001\n",
      "Train Epoch: 80 \tLoss eqn: 0.0497051477 Loss data: 0.0102068232\n",
      "learning rate is  0.001\n",
      "Train Epoch: 90 \tLoss eqn: 0.0528935790 Loss data: 0.0098756254\n",
      "learning rate is  0.001\n",
      "Train Epoch: 100 \tLoss eqn: 0.0523123331 Loss data: 0.0099304952\n",
      "learning rate is  0.001\n",
      "Train Epoch: 110 \tLoss eqn: 0.0514845401 Loss data: 0.0100096148\n",
      "learning rate is  0.001\n",
      "Train Epoch: 120 \tLoss eqn: 0.0519686155 Loss data: 0.0099580996\n",
      "learning rate is  0.001\n",
      "Train Epoch: 130 \tLoss eqn: 0.0520545766 Loss data: 0.0099469293\n",
      "learning rate is  0.001\n",
      "Train Epoch: 140 \tLoss eqn: 0.0519318581 Loss data: 0.0099568618\n",
      "learning rate is  0.001\n",
      "Train Epoch: 150 \tLoss eqn: 0.0518680513 Loss data: 0.0099610761\n",
      "learning rate is  0.001\n",
      "Train Epoch: 160 \tLoss eqn: 0.0518476777 Loss data: 0.0099610807\n",
      "learning rate is  0.001\n",
      "Train Epoch: 170 \tLoss eqn: 0.0518845730 Loss data: 0.0099554248\n",
      "learning rate is  0.001\n",
      "Train Epoch: 180 \tLoss eqn: 0.0518964678 Loss data: 0.0099523235\n",
      "learning rate is  0.001\n",
      "Train Epoch: 190 \tLoss eqn: 0.0518963858 Loss data: 0.0099503901\n",
      "learning rate is  0.001\n",
      "Train Epoch: 200 \tLoss eqn: 0.0519019775 Loss data: 0.0099478820\n",
      "learning rate is  0.001\n",
      "Train Epoch: 210 \tLoss eqn: 0.0519044399 Loss data: 0.0099456590\n",
      "learning rate is  0.001\n",
      "Train Epoch: 220 \tLoss eqn: 0.0519104451 Loss data: 0.0099430177\n",
      "learning rate is  0.001\n",
      "Train Epoch: 230 \tLoss eqn: 0.0519161597 Loss data: 0.0099403430\n",
      "learning rate is  0.001\n",
      "Train Epoch: 240 \tLoss eqn: 0.0519242361 Loss data: 0.0099373870\n",
      "learning rate is  0.001\n",
      "Train Epoch: 250 \tLoss eqn: 0.0519332550 Loss data: 0.0099342763\n",
      "learning rate is  0.001\n",
      "Train Epoch: 260 \tLoss eqn: 0.0519434623 Loss data: 0.0099309757\n",
      "learning rate is  0.001\n",
      "Train Epoch: 270 \tLoss eqn: 0.0519546792 Loss data: 0.0099275466\n",
      "learning rate is  0.001\n",
      "Train Epoch: 280 \tLoss eqn: 0.0519664288 Loss data: 0.0099240085\n",
      "learning rate is  0.001\n",
      "Train Epoch: 290 \tLoss eqn: 0.0519787855 Loss data: 0.0099203698\n",
      "learning rate is  0.001\n",
      "Train Epoch: 300 \tLoss eqn: 0.0519923754 Loss data: 0.0099165766\n",
      "learning rate is  0.001\n",
      "Train Epoch: 310 \tLoss eqn: 0.0520068780 Loss data: 0.0099126808\n",
      "learning rate is  0.001\n",
      "Train Epoch: 320 \tLoss eqn: 0.0520226769 Loss data: 0.0099086352\n",
      "learning rate is  0.001\n",
      "Train Epoch: 330 \tLoss eqn: 0.0520526879 Loss data: 0.0099032065\n",
      "learning rate is  0.001\n",
      "Train Epoch: 340 \tLoss eqn: 0.0522317477 Loss data: 0.0108958110\n",
      "learning rate is  0.001\n",
      "Train Epoch: 350 \tLoss eqn: 0.0510781668 Loss data: 0.0100274235\n",
      "learning rate is  0.001\n",
      "Train Epoch: 360 \tLoss eqn: 0.0526453108 Loss data: 0.0099072633\n",
      "learning rate is  0.001\n",
      "Train Epoch: 370 \tLoss eqn: 0.0518867858 Loss data: 0.0099520981\n",
      "learning rate is  0.001\n",
      "Train Epoch: 380 \tLoss eqn: 0.0519905500 Loss data: 0.0099000214\n",
      "learning rate is  0.001\n",
      "Train Epoch: 390 \tLoss eqn: 0.0522342436 Loss data: 0.0098733492\n",
      "learning rate is  0.001\n",
      "Train Epoch: 400 \tLoss eqn: 0.0521091223 Loss data: 0.0098833796\n",
      "learning rate is  0.001\n",
      "Train Epoch: 410 \tLoss eqn: 0.0522058792 Loss data: 0.0098706894\n",
      "learning rate is  0.001\n",
      "Train Epoch: 420 \tLoss eqn: 0.0522267148 Loss data: 0.0098661967\n",
      "learning rate is  0.001\n",
      "Train Epoch: 430 \tLoss eqn: 0.0522257648 Loss data: 0.0098642074\n",
      "learning rate is  0.001\n",
      "Train Epoch: 440 \tLoss eqn: 0.0522798821 Loss data: 0.0098565500\n",
      "learning rate is  0.001\n",
      "Train Epoch: 450 \tLoss eqn: 0.0523089580 Loss data: 0.0098515041\n",
      "learning rate is  0.001\n",
      "Train Epoch: 460 \tLoss eqn: 0.0523401685 Loss data: 0.0098462459\n",
      "learning rate is  0.001\n",
      "Train Epoch: 470 \tLoss eqn: 0.0523716398 Loss data: 0.0098409466\n",
      "learning rate is  0.001\n",
      "Train Epoch: 480 \tLoss eqn: 0.0524017401 Loss data: 0.0098357778\n",
      "learning rate is  0.001\n",
      "Train Epoch: 490 \tLoss eqn: 0.0524331555 Loss data: 0.0098304329\n",
      "learning rate is  0.001\n",
      "Train Epoch: 500 \tLoss eqn: 0.0524609946 Loss data: 0.0098253991\n",
      "learning rate is  0.001\n",
      "Train Epoch: 510 \tLoss eqn: 0.0524889939 Loss data: 0.0098203523\n",
      "learning rate is  0.001\n",
      "Train Epoch: 520 \tLoss eqn: 0.0525169633 Loss data: 0.0098153111\n",
      "learning rate is  0.001\n",
      "Train Epoch: 530 \tLoss eqn: 0.0525442027 Loss data: 0.0098103471\n",
      "learning rate is  0.001\n",
      "Train Epoch: 540 \tLoss eqn: 0.0525742248 Loss data: 0.0098051466\n",
      "learning rate is  0.001\n",
      "Train Epoch: 550 \tLoss eqn: 0.0538285896 Loss data: 0.0097408462\n",
      "learning rate is  0.001\n",
      "Train Epoch: 560 \tLoss eqn: 0.0524802804 Loss data: 0.0099995993\n",
      "learning rate is  0.001\n",
      "Train Epoch: 570 \tLoss eqn: 0.0509644672 Loss data: 0.0100942645\n",
      "learning rate is  0.001\n",
      "Train Epoch: 580 \tLoss eqn: 0.0518277027 Loss data: 0.0098863579\n",
      "learning rate is  0.001\n",
      "Train Epoch: 590 \tLoss eqn: 0.0522621535 Loss data: 0.0098315831\n",
      "learning rate is  0.001\n",
      "Train Epoch: 600 \tLoss eqn: 0.0523051918 Loss data: 0.0098311864\n",
      "learning rate is  0.001\n",
      "Train Epoch: 610 \tLoss eqn: 0.0525970683 Loss data: 0.0097951144\n",
      "learning rate is  0.001\n",
      "Train Epoch: 620 \tLoss eqn: 0.0526454784 Loss data: 0.0097890310\n",
      "learning rate is  0.001\n",
      "Train Epoch: 630 \tLoss eqn: 0.0526938885 Loss data: 0.0097821411\n",
      "learning rate is  0.001\n",
      "Train Epoch: 640 \tLoss eqn: 0.0527729578 Loss data: 0.0097729573\n",
      "learning rate is  0.001\n",
      "Train Epoch: 650 \tLoss eqn: 0.0527125932 Loss data: 0.0097778020\n",
      "learning rate is  0.001\n",
      "Train Epoch: 660 \tLoss eqn: 0.0527089015 Loss data: 0.0097771315\n",
      "learning rate is  0.001\n",
      "Train Epoch: 670 \tLoss eqn: 0.0527122021 Loss data: 0.0097758453\n",
      "learning rate is  0.001\n",
      "Train Epoch: 680 \tLoss eqn: 0.0527081490 Loss data: 0.0097753610\n",
      "learning rate is  0.001\n",
      "Train Epoch: 690 \tLoss eqn: 0.0527167581 Loss data: 0.0097736772\n",
      "learning rate is  0.001\n",
      "Train Epoch: 700 \tLoss eqn: 0.0527171269 Loss data: 0.0097728912\n",
      "learning rate is  0.001\n",
      "Train Epoch: 710 \tLoss eqn: 0.0527271368 Loss data: 0.0097711999\n",
      "learning rate is  0.001\n",
      "Train Epoch: 720 \tLoss eqn: 0.0528166220 Loss data: 0.0097619686\n",
      "learning rate is  0.001\n",
      "Train Epoch: 730 \tLoss eqn: 0.0515098386 Loss data: 0.0100268563\n",
      "learning rate is  0.001\n",
      "Train Epoch: 740 \tLoss eqn: 0.0489601307 Loss data: 0.0102843866\n",
      "learning rate is  0.001\n",
      "Train Epoch: 750 \tLoss eqn: 0.0560190082 Loss data: 0.0094902627\n",
      "learning rate is  0.001\n",
      "Train Epoch: 760 \tLoss eqn: 0.0518151782 Loss data: 0.0098792864\n",
      "learning rate is  0.001\n",
      "Train Epoch: 770 \tLoss eqn: 0.0532153100 Loss data: 0.0097265691\n",
      "learning rate is  0.001\n",
      "Train Epoch: 780 \tLoss eqn: 0.0531923920 Loss data: 0.0097228326\n",
      "learning rate is  0.001\n",
      "Train Epoch: 790 \tLoss eqn: 0.0523827337 Loss data: 0.0098018982\n",
      "learning rate is  0.001\n",
      "Train Epoch: 800 \tLoss eqn: 0.0524644852 Loss data: 0.0097925402\n",
      "learning rate is  0.001\n",
      "Train Epoch: 810 \tLoss eqn: 0.0525942408 Loss data: 0.0097784977\n",
      "learning rate is  0.001\n",
      "Train Epoch: 820 \tLoss eqn: 0.0527955890 Loss data: 0.0097571854\n",
      "learning rate is  0.001\n",
      "Train Epoch: 830 \tLoss eqn: 0.0527698658 Loss data: 0.0097582815\n",
      "learning rate is  0.001\n",
      "Train Epoch: 840 \tLoss eqn: 0.0528184213 Loss data: 0.0097515350\n",
      "learning rate is  0.001\n",
      "Train Epoch: 850 \tLoss eqn: 0.0528337620 Loss data: 0.0097473925\n",
      "learning rate is  0.001\n",
      "Train Epoch: 860 \tLoss eqn: 0.0525650680 Loss data: 0.0097758360\n",
      "learning rate is  0.001\n",
      "Train Epoch: 870 \tLoss eqn: 0.0443971902 Loss data: 0.0108839199\n",
      "learning rate is  0.001\n",
      "Train Epoch: 880 \tLoss eqn: 0.0629388094 Loss data: 0.0088417474\n",
      "learning rate is  0.001\n",
      "Train Epoch: 890 \tLoss eqn: 0.0586621836 Loss data: 0.0092061739\n",
      "learning rate is  0.001\n",
      "Train Epoch: 900 \tLoss eqn: 0.0561321378 Loss data: 0.0094257388\n",
      "learning rate is  0.001\n",
      "Train Epoch: 910 \tLoss eqn: 0.0543316528 Loss data: 0.0095890239\n",
      "learning rate is  0.001\n",
      "Train Epoch: 920 \tLoss eqn: 0.0522289835 Loss data: 0.0097889975\n",
      "learning rate is  0.001\n",
      "Train Epoch: 930 \tLoss eqn: 0.0528818890 Loss data: 0.0097116288\n",
      "learning rate is  0.001\n",
      "Train Epoch: 940 \tLoss eqn: 0.0523279943 Loss data: 0.0097519234\n",
      "learning rate is  0.001\n",
      "Train Epoch: 950 \tLoss eqn: 0.0526000969 Loss data: 0.0097080618\n",
      "learning rate is  0.001\n",
      "Train Epoch: 960 \tLoss eqn: 0.0561225228 Loss data: 0.0093783429\n",
      "learning rate is  0.001\n",
      "Train Epoch: 970 \tLoss eqn: 0.0490351990 Loss data: 0.0101752691\n",
      "learning rate is  0.001\n",
      "Train Epoch: 980 \tLoss eqn: 0.0451317616 Loss data: 0.0105235903\n",
      "learning rate is  0.001\n",
      "Train Epoch: 990 \tLoss eqn: 0.0520609543 Loss data: 0.0097684991\n",
      "learning rate is  0.001\n",
      "Train Epoch: 1000 \tLoss eqn: 0.0542517118 Loss data: 0.0095295412\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1010 \tLoss eqn: 0.0527825728 Loss data: 0.0096694343\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1020 \tLoss eqn: 0.0531248599 Loss data: 0.0096321432\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1030 \tLoss eqn: 0.0530527830 Loss data: 0.0096368464\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1040 \tLoss eqn: 0.0529697947 Loss data: 0.0096432120\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1050 \tLoss eqn: 0.0530809201 Loss data: 0.0096303951\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1060 \tLoss eqn: 0.0531114675 Loss data: 0.0096258838\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1070 \tLoss eqn: 0.0530633591 Loss data: 0.0096293483\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1080 \tLoss eqn: 0.0530600958 Loss data: 0.0096284430\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1090 \tLoss eqn: 0.0530497432 Loss data: 0.0096283406\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1100 \tLoss eqn: 0.0530547313 Loss data: 0.0096267853\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1110 \tLoss eqn: 0.0530634783 Loss data: 0.0096249711\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1120 \tLoss eqn: 0.0530664995 Loss data: 0.0096238088\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1130 \tLoss eqn: 0.0530627668 Loss data: 0.0096234363\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1140 \tLoss eqn: 0.0530662909 Loss data: 0.0096224416\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1150 \tLoss eqn: 0.0530684926 Loss data: 0.0096216304\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1160 \tLoss eqn: 0.0530710556 Loss data: 0.0096208826\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1170 \tLoss eqn: 0.0530737154 Loss data: 0.0096201599\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1180 \tLoss eqn: 0.0530768372 Loss data: 0.0096194483\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1190 \tLoss eqn: 0.0530799776 Loss data: 0.0096186996\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1200 \tLoss eqn: 0.0530832261 Loss data: 0.0096179321\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1210 \tLoss eqn: 0.0530866832 Loss data: 0.0096171265\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1220 \tLoss eqn: 0.0530900471 Loss data: 0.0096163023\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1230 \tLoss eqn: 0.0530933999 Loss data: 0.0096154381\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1240 \tLoss eqn: 0.0530969054 Loss data: 0.0096144956\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1250 \tLoss eqn: 0.0531008691 Loss data: 0.0096134692\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1260 \tLoss eqn: 0.0531050302 Loss data: 0.0096124019\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1270 \tLoss eqn: 0.0531094745 Loss data: 0.0096112369\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1280 \tLoss eqn: 0.0531143546 Loss data: 0.0096099889\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1290 \tLoss eqn: 0.0531197377 Loss data: 0.0096086552\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1300 \tLoss eqn: 0.0531253703 Loss data: 0.0096072722\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1310 \tLoss eqn: 0.0531318747 Loss data: 0.0096057979\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1320 \tLoss eqn: 0.0531385504 Loss data: 0.0096043274\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1330 \tLoss eqn: 0.0531457253 Loss data: 0.0096028335\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1340 \tLoss eqn: 0.0531522818 Loss data: 0.0096014654\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1350 \tLoss eqn: 0.0531589650 Loss data: 0.0096001327\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1360 \tLoss eqn: 0.0531647243 Loss data: 0.0095989369\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1370 \tLoss eqn: 0.0531681627 Loss data: 0.0095979720\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1380 \tLoss eqn: 0.0531651862 Loss data: 0.0095976032\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1390 \tLoss eqn: 0.0514150746 Loss data: 0.0097779278\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1400 \tLoss eqn: 0.0548259206 Loss data: 0.0094343824\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1410 \tLoss eqn: 0.0537385829 Loss data: 0.0095388908\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1420 \tLoss eqn: 0.0530712828 Loss data: 0.0096043814\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1430 \tLoss eqn: 0.0531591401 Loss data: 0.0095941909\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1440 \tLoss eqn: 0.0528230481 Loss data: 0.0096267434\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1450 \tLoss eqn: 0.0514294095 Loss data: 0.0097700767\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1460 \tLoss eqn: 0.0521858297 Loss data: 0.0096888635\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1470 \tLoss eqn: 0.0543662831 Loss data: 0.0094698984\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1480 \tLoss eqn: 0.0538850240 Loss data: 0.0095153898\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1490 \tLoss eqn: 0.0546885096 Loss data: 0.0094358353\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1500 \tLoss eqn: 0.0554181337 Loss data: 0.0093629537\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1510 \tLoss eqn: 0.0540001914 Loss data: 0.0094999624\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1520 \tLoss eqn: 0.0527232736 Loss data: 0.0096246637\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1530 \tLoss eqn: 0.0545237809 Loss data: 0.0094449129\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1540 \tLoss eqn: 0.0528294221 Loss data: 0.0096063521\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1550 \tLoss eqn: 0.0547971427 Loss data: 0.0094111580\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1560 \tLoss eqn: 0.0529727936 Loss data: 0.0095840609\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1570 \tLoss eqn: 0.0520872958 Loss data: 0.0096671758\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1580 \tLoss eqn: 0.0530823693 Loss data: 0.0095518772\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1590 \tLoss eqn: 0.0507843681 Loss data: 0.0097631253\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1600 \tLoss eqn: 0.0560588762 Loss data: 0.0092302095\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1610 \tLoss eqn: 0.0557625555 Loss data: 0.0091814129\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1620 \tLoss eqn: 0.0547409989 Loss data: 0.0091856699\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1630 \tLoss eqn: 0.0532564148 Loss data: 0.0097139589\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1640 \tLoss eqn: 0.0533462986 Loss data: 0.0096156020\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1650 \tLoss eqn: 0.0534132868 Loss data: 0.0095881335\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1660 \tLoss eqn: 0.0529238209 Loss data: 0.0096290559\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1670 \tLoss eqn: 0.0531842597 Loss data: 0.0095980382\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1680 \tLoss eqn: 0.0530487336 Loss data: 0.0096080210\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1690 \tLoss eqn: 0.0529182181 Loss data: 0.0096180653\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1700 \tLoss eqn: 0.0530085824 Loss data: 0.0096065169\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1710 \tLoss eqn: 0.0529588722 Loss data: 0.0096091479\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1720 \tLoss eqn: 0.0531061031 Loss data: 0.0095920581\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1730 \tLoss eqn: 0.0530487373 Loss data: 0.0095952861\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1740 \tLoss eqn: 0.0556911081 Loss data: 0.0093577374\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1750 \tLoss eqn: 0.0515344329 Loss data: 0.0097532384\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1760 \tLoss eqn: 0.0525034815 Loss data: 0.0096428189\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1770 \tLoss eqn: 0.0533581786 Loss data: 0.0095523829\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1780 \tLoss eqn: 0.0532963276 Loss data: 0.0095537193\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1790 \tLoss eqn: 0.0528897271 Loss data: 0.0095881354\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1800 \tLoss eqn: 0.0529299714 Loss data: 0.0095756985\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1810 \tLoss eqn: 0.0528924465 Loss data: 0.0095671788\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1820 \tLoss eqn: 0.0523570627 Loss data: 0.0096015967\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1830 \tLoss eqn: 0.0515395924 Loss data: 0.0096536204\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1840 \tLoss eqn: 0.0558385924 Loss data: 0.0091760196\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1850 \tLoss eqn: 0.0526391678 Loss data: 0.0093829278\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1860 \tLoss eqn: 0.0476015322 Loss data: 0.0097670220\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1870 \tLoss eqn: 0.0471571051 Loss data: 0.0094099147\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1880 \tLoss eqn: 0.0470669158 Loss data: 0.0106536476\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1890 \tLoss eqn: 0.0568629541 Loss data: 0.0093581881\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1900 \tLoss eqn: 0.0505161695 Loss data: 0.0099238558\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1910 \tLoss eqn: 0.0525227636 Loss data: 0.0097063519\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1920 \tLoss eqn: 0.0528059416 Loss data: 0.0096663777\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1930 \tLoss eqn: 0.0529823862 Loss data: 0.0096414862\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1940 \tLoss eqn: 0.0528336093 Loss data: 0.0096500758\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1950 \tLoss eqn: 0.0527390242 Loss data: 0.0096543022\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1960 \tLoss eqn: 0.0528821982 Loss data: 0.0096351132\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1970 \tLoss eqn: 0.0529696420 Loss data: 0.0096220700\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1980 \tLoss eqn: 0.0529499799 Loss data: 0.0096204644\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 1990 \tLoss eqn: 0.0529791377 Loss data: 0.0096147191\n",
      "learning rate is  0.0002\n",
      "Train Epoch: 2000 \tLoss eqn: 0.0534345098 Loss data: 0.0095690563\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2010 \tLoss eqn: 0.0527860932 Loss data: 0.0096320566\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2020 \tLoss eqn: 0.0531145260 Loss data: 0.0095983027\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2030 \tLoss eqn: 0.0530365035 Loss data: 0.0096056163\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2040 \tLoss eqn: 0.0529756248 Loss data: 0.0096113337\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2050 \tLoss eqn: 0.0530473590 Loss data: 0.0096037425\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2060 \tLoss eqn: 0.0530504137 Loss data: 0.0096030114\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2070 \tLoss eqn: 0.0530343130 Loss data: 0.0096041821\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2080 \tLoss eqn: 0.0530322269 Loss data: 0.0096039688\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2090 \tLoss eqn: 0.0530356094 Loss data: 0.0096031595\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2100 \tLoss eqn: 0.0530399084 Loss data: 0.0096022924\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2110 \tLoss eqn: 0.0530436635 Loss data: 0.0096014421\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2120 \tLoss eqn: 0.0530473143 Loss data: 0.0096006114\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2130 \tLoss eqn: 0.0530501083 Loss data: 0.0095998151\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2140 \tLoss eqn: 0.0530519858 Loss data: 0.0095991148\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2150 \tLoss eqn: 0.0530539826 Loss data: 0.0095983949\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2160 \tLoss eqn: 0.0530566536 Loss data: 0.0095975846\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2170 \tLoss eqn: 0.0530587994 Loss data: 0.0095968163\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2180 \tLoss eqn: 0.0530609898 Loss data: 0.0095960237\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2190 \tLoss eqn: 0.0530630425 Loss data: 0.0095952107\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2200 \tLoss eqn: 0.0530651808 Loss data: 0.0095943790\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2210 \tLoss eqn: 0.0530672818 Loss data: 0.0095935017\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2220 \tLoss eqn: 0.0530691147 Loss data: 0.0095926542\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2230 \tLoss eqn: 0.0530709624 Loss data: 0.0095917713\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2240 \tLoss eqn: 0.0530726984 Loss data: 0.0095908474\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2250 \tLoss eqn: 0.0530741401 Loss data: 0.0095899403\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2260 \tLoss eqn: 0.0530756265 Loss data: 0.0095889941\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2270 \tLoss eqn: 0.0530763082 Loss data: 0.0095880665\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2280 \tLoss eqn: 0.0530778579 Loss data: 0.0095870076\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2290 \tLoss eqn: 0.0530755520 Loss data: 0.0095862970\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2300 \tLoss eqn: 0.0526226759 Loss data: 0.0096324449\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2310 \tLoss eqn: 0.0532158166 Loss data: 0.0095704217\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2320 \tLoss eqn: 0.0529032610 Loss data: 0.0096008778\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2330 \tLoss eqn: 0.0531666875 Loss data: 0.0095731430\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2340 \tLoss eqn: 0.0530163087 Loss data: 0.0095869573\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2350 \tLoss eqn: 0.0531082302 Loss data: 0.0095764389\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2360 \tLoss eqn: 0.0531043410 Loss data: 0.0095754238\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2370 \tLoss eqn: 0.0530455373 Loss data: 0.0095797703\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2380 \tLoss eqn: 0.0529660024 Loss data: 0.0095861387\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2390 \tLoss eqn: 0.0534180142 Loss data: 0.0095402990\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2400 \tLoss eqn: 0.0528744720 Loss data: 0.0095920162\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2410 \tLoss eqn: 0.0530671887 Loss data: 0.0095701925\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2420 \tLoss eqn: 0.0531389639 Loss data: 0.0095607564\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2430 \tLoss eqn: 0.0530360602 Loss data: 0.0095683895\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2440 \tLoss eqn: 0.0532569773 Loss data: 0.0095436852\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2450 \tLoss eqn: 0.0527560972 Loss data: 0.0095909592\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2460 \tLoss eqn: 0.0529794581 Loss data: 0.0095641408\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2470 \tLoss eqn: 0.0532043129 Loss data: 0.0095376298\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2480 \tLoss eqn: 0.0532317236 Loss data: 0.0095299343\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2490 \tLoss eqn: 0.0525043868 Loss data: 0.0095990067\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2500 \tLoss eqn: 0.0526314452 Loss data: 0.0095785577\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2510 \tLoss eqn: 0.0531977303 Loss data: 0.0095146764\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2520 \tLoss eqn: 0.0525964238 Loss data: 0.0095671453\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2530 \tLoss eqn: 0.0527769364 Loss data: 0.0095380535\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2540 \tLoss eqn: 0.0523602813 Loss data: 0.0095688514\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2550 \tLoss eqn: 0.0528238229 Loss data: 0.0095087318\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2560 \tLoss eqn: 0.0528242067 Loss data: 0.0094913878\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2570 \tLoss eqn: 0.0509532355 Loss data: 0.0096664978\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2580 \tLoss eqn: 0.0515705608 Loss data: 0.0095802303\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2590 \tLoss eqn: 0.0524472892 Loss data: 0.0094671277\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2600 \tLoss eqn: 0.0523353741 Loss data: 0.0094472505\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2610 \tLoss eqn: 0.0527602956 Loss data: 0.0093630087\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2620 \tLoss eqn: 0.0514964387 Loss data: 0.0094393753\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2630 \tLoss eqn: 0.0515489727 Loss data: 0.0093646348\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2640 \tLoss eqn: 0.0522133708 Loss data: 0.0092065996\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2650 \tLoss eqn: 0.0516456626 Loss data: 0.0091331769\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2660 \tLoss eqn: 0.0483414680 Loss data: 0.0093120709\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2670 \tLoss eqn: 0.0507569127 Loss data: 0.0088683479\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2680 \tLoss eqn: 0.0502427295 Loss data: 0.0086911405\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2690 \tLoss eqn: 0.0497137532 Loss data: 0.0084101278\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2700 \tLoss eqn: 0.0511487462 Loss data: 0.0078368215\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2710 \tLoss eqn: 0.0472717211 Loss data: 0.0076422608\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2720 \tLoss eqn: 0.0472464636 Loss data: 0.0069619836\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2730 \tLoss eqn: 0.0403154716 Loss data: 0.0067930389\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2740 \tLoss eqn: 0.0381136499 Loss data: 0.0061156168\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2750 \tLoss eqn: 0.0336227417 Loss data: 0.0054988055\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2760 \tLoss eqn: 0.0357026607 Loss data: 0.0042743422\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2770 \tLoss eqn: 0.0307633970 Loss data: 0.0036197607\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2780 \tLoss eqn: 0.0240545403 Loss data: 0.0033528151\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2790 \tLoss eqn: 0.0201227758 Loss data: 0.0027199143\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2800 \tLoss eqn: 0.0190722533 Loss data: 0.0018491297\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2810 \tLoss eqn: 0.0179042518 Loss data: 0.0012654320\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2820 \tLoss eqn: 0.0123114632 Loss data: 0.0010461793\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2830 \tLoss eqn: 0.0093832919 Loss data: 0.0009318734\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2840 \tLoss eqn: 0.0095119905 Loss data: 0.0004488411\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2850 \tLoss eqn: 0.0077184848 Loss data: 0.0002862709\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2860 \tLoss eqn: 0.0045306901 Loss data: 0.0002990702\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2870 \tLoss eqn: 0.0037104872 Loss data: 0.0001683201\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2880 \tLoss eqn: 0.0030837797 Loss data: 0.0000973533\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2890 \tLoss eqn: 0.0022590191 Loss data: 0.0000852615\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2900 \tLoss eqn: 0.0018761714 Loss data: 0.0000461041\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2910 \tLoss eqn: 0.0015406418 Loss data: 0.0000290954\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2920 \tLoss eqn: 0.0012369324 Loss data: 0.0000293165\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2930 \tLoss eqn: 0.0014739154 Loss data: 0.0000045939\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2940 \tLoss eqn: 0.0012714433 Loss data: 0.0000300659\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2950 \tLoss eqn: 0.0010644618 Loss data: 0.0000016179\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2960 \tLoss eqn: 0.0006497990 Loss data: 0.0000067922\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2970 \tLoss eqn: 0.0006909010 Loss data: 0.0000127450\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2980 \tLoss eqn: 0.0010917288 Loss data: 0.0000002739\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 2990 \tLoss eqn: 0.0006753308 Loss data: 0.0000114388\n",
      "learning rate is  4e-05\n",
      "Train Epoch: 3000 \tLoss eqn: 0.0004327404 Loss data: 0.0000015056\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3010 \tLoss eqn: 0.0003986384 Loss data: 0.0000026422\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3020 \tLoss eqn: 0.0003962907 Loss data: 0.0000014766\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3030 \tLoss eqn: 0.0003816874 Loss data: 0.0000018413\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3040 \tLoss eqn: 0.0003742943 Loss data: 0.0000016238\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3050 \tLoss eqn: 0.0003668107 Loss data: 0.0000014624\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3060 \tLoss eqn: 0.0003572728 Loss data: 0.0000014853\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3070 \tLoss eqn: 0.0003487894 Loss data: 0.0000014001\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3080 \tLoss eqn: 0.0003403793 Loss data: 0.0000013072\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3090 \tLoss eqn: 0.0003316881 Loss data: 0.0000012365\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3100 \tLoss eqn: 0.0003229232 Loss data: 0.0000011729\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3110 \tLoss eqn: 0.0003141210 Loss data: 0.0000011127\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3120 \tLoss eqn: 0.0003053474 Loss data: 0.0000010526\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3130 \tLoss eqn: 0.0002965092 Loss data: 0.0000009968\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3140 \tLoss eqn: 0.0002877276 Loss data: 0.0000009435\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3150 \tLoss eqn: 0.0002790600 Loss data: 0.0000008879\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3160 \tLoss eqn: 0.0002705111 Loss data: 0.0000008325\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3170 \tLoss eqn: 0.0002620328 Loss data: 0.0000007796\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3180 \tLoss eqn: 0.0002535457 Loss data: 0.0000007346\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3190 \tLoss eqn: 0.0002453246 Loss data: 0.0000006843\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3200 \tLoss eqn: 0.0002372377 Loss data: 0.0000006401\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3210 \tLoss eqn: 0.0002293504 Loss data: 0.0000005938\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3220 \tLoss eqn: 0.0002215510 Loss data: 0.0000005534\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3230 \tLoss eqn: 0.0002139885 Loss data: 0.0000005139\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3240 \tLoss eqn: 0.0002066411 Loss data: 0.0000004770\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3250 \tLoss eqn: 0.0001995289 Loss data: 0.0000004392\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3260 \tLoss eqn: 0.0001926500 Loss data: 0.0000004038\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3270 \tLoss eqn: 0.0001859638 Loss data: 0.0000003776\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3280 \tLoss eqn: 0.0001795374 Loss data: 0.0000003481\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3290 \tLoss eqn: 0.0001734654 Loss data: 0.0000003172\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3300 \tLoss eqn: 0.0001675366 Loss data: 0.0000002916\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3310 \tLoss eqn: 0.0001618125 Loss data: 0.0000002714\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3320 \tLoss eqn: 0.0001564739 Loss data: 0.0000002470\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3330 \tLoss eqn: 0.0001511247 Loss data: 0.0000003042\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3340 \tLoss eqn: 0.0001463620 Loss data: 0.0000003203\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3350 \tLoss eqn: 0.0001493996 Loss data: 0.0000005388\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3360 \tLoss eqn: 0.0001433380 Loss data: 0.0000004671\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3370 \tLoss eqn: 0.0001401305 Loss data: 0.0000000523\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3380 \tLoss eqn: 0.0001339842 Loss data: 0.0000003169\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3390 \tLoss eqn: 0.0001306749 Loss data: 0.0000003549\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3400 \tLoss eqn: 0.0001300910 Loss data: 0.0000003794\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3410 \tLoss eqn: 0.0001280094 Loss data: 0.0000003820\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3420 \tLoss eqn: 0.0001214749 Loss data: 0.0000001979\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3430 \tLoss eqn: 0.0001188642 Loss data: 0.0000000963\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3440 \tLoss eqn: 0.0001634675 Loss data: 0.0000001592\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3450 \tLoss eqn: 0.0001146713 Loss data: 0.0000002336\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3460 \tLoss eqn: 0.0001120848 Loss data: 0.0000001506\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3470 \tLoss eqn: 0.0001121917 Loss data: 0.0000002426\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3480 \tLoss eqn: 0.0001078878 Loss data: 0.0000001127\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3490 \tLoss eqn: 0.0001077024 Loss data: 0.0000002100\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3500 \tLoss eqn: 0.0001544308 Loss data: 0.0000012170\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3510 \tLoss eqn: 0.0001026982 Loss data: 0.0000000925\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3520 \tLoss eqn: 0.0001099746 Loss data: 0.0000000203\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3530 \tLoss eqn: 0.0000989328 Loss data: 0.0000000647\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3540 \tLoss eqn: 0.0001013050 Loss data: 0.0000000070\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3550 \tLoss eqn: 0.0001223153 Loss data: 0.0000001098\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3560 \tLoss eqn: 0.0001201336 Loss data: 0.0000008633\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3570 \tLoss eqn: 0.0001007840 Loss data: 0.0000003354\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3580 \tLoss eqn: 0.0000918888 Loss data: 0.0000000971\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3590 \tLoss eqn: 0.0000914719 Loss data: 0.0000000242\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3600 \tLoss eqn: 0.0001135677 Loss data: 0.0000000969\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3610 \tLoss eqn: 0.0000987661 Loss data: 0.0000003824\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3620 \tLoss eqn: 0.0000958873 Loss data: 0.0000000319\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3630 \tLoss eqn: 0.0000851649 Loss data: 0.0000000327\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3640 \tLoss eqn: 0.0001244756 Loss data: 0.0000008825\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3650 \tLoss eqn: 0.0000826133 Loss data: 0.0000000888\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3660 \tLoss eqn: 0.0000874320 Loss data: 0.0000002258\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3670 \tLoss eqn: 0.0000837323 Loss data: 0.0000001806\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3680 \tLoss eqn: 0.0000791928 Loss data: 0.0000000400\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3690 \tLoss eqn: 0.0000789153 Loss data: 0.0000000226\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3700 \tLoss eqn: 0.0000992472 Loss data: 0.0000001269\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3710 \tLoss eqn: 0.0000957633 Loss data: 0.0000006180\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3720 \tLoss eqn: 0.0000775866 Loss data: 0.0000000548\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3730 \tLoss eqn: 0.0000751730 Loss data: 0.0000000231\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3740 \tLoss eqn: 0.0000844231 Loss data: 0.0000000765\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3750 \tLoss eqn: 0.0000779363 Loss data: 0.0000000276\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3760 \tLoss eqn: 0.0000732743 Loss data: 0.0000001056\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3770 \tLoss eqn: 0.0000873185 Loss data: 0.0000000730\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3780 \tLoss eqn: 0.0000953733 Loss data: 0.0000001620\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3790 \tLoss eqn: 0.0000692544 Loss data: 0.0000000466\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3800 \tLoss eqn: 0.0000690498 Loss data: 0.0000000825\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3810 \tLoss eqn: 0.0000671785 Loss data: 0.0000000293\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3820 \tLoss eqn: 0.0000672398 Loss data: 0.0000000069\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3830 \tLoss eqn: 0.0000715971 Loss data: 0.0000000171\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3840 \tLoss eqn: 0.0000663637 Loss data: 0.0000001193\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3850 \tLoss eqn: 0.0000651322 Loss data: 0.0000000079\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3860 \tLoss eqn: 0.0000678604 Loss data: 0.0000000195\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3870 \tLoss eqn: 0.0000644750 Loss data: 0.0000000569\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3880 \tLoss eqn: 0.0000636837 Loss data: 0.0000000916\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3890 \tLoss eqn: 0.0001465134 Loss data: 0.0000007264\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3900 \tLoss eqn: 0.0000957526 Loss data: 0.0000002768\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3910 \tLoss eqn: 0.0000603529 Loss data: 0.0000000050\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3920 \tLoss eqn: 0.0000618924 Loss data: 0.0000001059\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3930 \tLoss eqn: 0.0000595926 Loss data: 0.0000000868\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3940 \tLoss eqn: 0.0000583789 Loss data: 0.0000000060\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3950 \tLoss eqn: 0.0000582420 Loss data: 0.0000000623\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3960 \tLoss eqn: 0.0001246285 Loss data: 0.0000012394\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3970 \tLoss eqn: 0.0000688058 Loss data: 0.0000003252\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3980 \tLoss eqn: 0.0000609070 Loss data: 0.0000001695\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 3990 \tLoss eqn: 0.0000549202 Loss data: 0.0000000273\n",
      "learning rate is  8.000000000000001e-06\n",
      "Train Epoch: 4000 \tLoss eqn: 0.0000569727 Loss data: 0.0000000087\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4010 \tLoss eqn: 0.0000547682 Loss data: 0.0000000467\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4020 \tLoss eqn: 0.0000544782 Loss data: 0.0000000038\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4030 \tLoss eqn: 0.0000539542 Loss data: 0.0000000226\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4040 \tLoss eqn: 0.0000538457 Loss data: 0.0000000100\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4050 \tLoss eqn: 0.0000536505 Loss data: 0.0000000130\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4060 \tLoss eqn: 0.0000534792 Loss data: 0.0000000144\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4070 \tLoss eqn: 0.0000533608 Loss data: 0.0000000119\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4080 \tLoss eqn: 0.0000531900 Loss data: 0.0000000122\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4090 \tLoss eqn: 0.0000530326 Loss data: 0.0000000128\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4100 \tLoss eqn: 0.0000528573 Loss data: 0.0000000125\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4110 \tLoss eqn: 0.0000526506 Loss data: 0.0000000126\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4120 \tLoss eqn: 0.0000524627 Loss data: 0.0000000125\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4130 \tLoss eqn: 0.0000522889 Loss data: 0.0000000123\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4140 \tLoss eqn: 0.0000520674 Loss data: 0.0000000123\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4150 \tLoss eqn: 0.0000518567 Loss data: 0.0000000122\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4160 \tLoss eqn: 0.0000516166 Loss data: 0.0000000124\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4170 \tLoss eqn: 0.0000513848 Loss data: 0.0000000119\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4180 \tLoss eqn: 0.0000511371 Loss data: 0.0000000116\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4190 \tLoss eqn: 0.0000508759 Loss data: 0.0000000122\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4200 \tLoss eqn: 0.0000506363 Loss data: 0.0000000112\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4210 \tLoss eqn: 0.0000503394 Loss data: 0.0000000114\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4220 \tLoss eqn: 0.0000500677 Loss data: 0.0000000111\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4230 \tLoss eqn: 0.0000497613 Loss data: 0.0000000115\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4240 \tLoss eqn: 0.0000494343 Loss data: 0.0000000114\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4250 \tLoss eqn: 0.0000491310 Loss data: 0.0000000112\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4260 \tLoss eqn: 0.0000487897 Loss data: 0.0000000110\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4270 \tLoss eqn: 0.0000484553 Loss data: 0.0000000104\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4280 \tLoss eqn: 0.0000480902 Loss data: 0.0000000102\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4290 \tLoss eqn: 0.0000477413 Loss data: 0.0000000101\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4300 \tLoss eqn: 0.0000473421 Loss data: 0.0000000114\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4310 \tLoss eqn: 0.0000469409 Loss data: 0.0000000108\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4320 \tLoss eqn: 0.0000465389 Loss data: 0.0000000108\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4330 \tLoss eqn: 0.0000461376 Loss data: 0.0000000081\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4340 \tLoss eqn: 0.0000467252 Loss data: 0.0000000011\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4350 \tLoss eqn: 0.0000454574 Loss data: 0.0000000229\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4360 \tLoss eqn: 0.0000459504 Loss data: 0.0000000038\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4370 \tLoss eqn: 0.0000447043 Loss data: 0.0000000190\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4380 \tLoss eqn: 0.0000443675 Loss data: 0.0000000149\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4390 \tLoss eqn: 0.0000439216 Loss data: 0.0000000137\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4400 \tLoss eqn: 0.0000435634 Loss data: 0.0000000093\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4410 \tLoss eqn: 0.0000432488 Loss data: 0.0000000179\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4420 \tLoss eqn: 0.0000448372 Loss data: 0.0000000718\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4430 \tLoss eqn: 0.0000430287 Loss data: 0.0000000362\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4440 \tLoss eqn: 0.0000422100 Loss data: 0.0000000186\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4450 \tLoss eqn: 0.0000419579 Loss data: 0.0000000037\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4460 \tLoss eqn: 0.0000414489 Loss data: 0.0000000121\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4470 \tLoss eqn: 0.0000416978 Loss data: 0.0000000008\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4480 \tLoss eqn: 0.0000421706 Loss data: 0.0000000576\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4490 \tLoss eqn: 0.0000404684 Loss data: 0.0000000150\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4500 \tLoss eqn: 0.0000401814 Loss data: 0.0000000069\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4510 \tLoss eqn: 0.0000398312 Loss data: 0.0000000076\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4520 \tLoss eqn: 0.0000394492 Loss data: 0.0000000096\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4530 \tLoss eqn: 0.0000391810 Loss data: 0.0000000155\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4540 \tLoss eqn: 0.0000390693 Loss data: 0.0000000243\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4550 \tLoss eqn: 0.0000386066 Loss data: 0.0000000099\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4560 \tLoss eqn: 0.0000382871 Loss data: 0.0000000111\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4570 \tLoss eqn: 0.0000380312 Loss data: 0.0000000139\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4580 \tLoss eqn: 0.0000378520 Loss data: 0.0000000026\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4590 \tLoss eqn: 0.0000376291 Loss data: 0.0000000017\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4600 \tLoss eqn: 0.0000373665 Loss data: 0.0000000231\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4610 \tLoss eqn: 0.0000376052 Loss data: 0.0000000362\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4620 \tLoss eqn: 0.0000366656 Loss data: 0.0000000146\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4630 \tLoss eqn: 0.0000363476 Loss data: 0.0000000089\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4640 \tLoss eqn: 0.0000360975 Loss data: 0.0000000112\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4650 \tLoss eqn: 0.0000358976 Loss data: 0.0000000026\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4660 \tLoss eqn: 0.0000355753 Loss data: 0.0000000130\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4670 \tLoss eqn: 0.0000375299 Loss data: 0.0000000094\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4680 \tLoss eqn: 0.0000350872 Loss data: 0.0000000029\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4690 \tLoss eqn: 0.0000350417 Loss data: 0.0000000220\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4700 \tLoss eqn: 0.0000344840 Loss data: 0.0000000062\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4710 \tLoss eqn: 0.0000342649 Loss data: 0.0000000040\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4720 \tLoss eqn: 0.0000339759 Loss data: 0.0000000057\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4730 \tLoss eqn: 0.0000338498 Loss data: 0.0000000045\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4740 \tLoss eqn: 0.0000344659 Loss data: 0.0000000020\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4750 \tLoss eqn: 0.0000335082 Loss data: 0.0000000197\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4760 \tLoss eqn: 0.0000334247 Loss data: 0.0000000019\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4770 \tLoss eqn: 0.0000328054 Loss data: 0.0000000101\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4780 \tLoss eqn: 0.0000347005 Loss data: 0.0000000582\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4790 \tLoss eqn: 0.0000326053 Loss data: 0.0000000179\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4800 \tLoss eqn: 0.0000321969 Loss data: 0.0000000035\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4810 \tLoss eqn: 0.0000319870 Loss data: 0.0000000032\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4820 \tLoss eqn: 0.0000317487 Loss data: 0.0000000079\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4830 \tLoss eqn: 0.0000315513 Loss data: 0.0000000032\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4840 \tLoss eqn: 0.0000313001 Loss data: 0.0000000047\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4850 \tLoss eqn: 0.0000312426 Loss data: 0.0000000159\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4860 \tLoss eqn: 0.0000340955 Loss data: 0.0000000853\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4870 \tLoss eqn: 0.0000307135 Loss data: 0.0000000078\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4880 \tLoss eqn: 0.0000306671 Loss data: 0.0000000009\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4890 \tLoss eqn: 0.0000306493 Loss data: 0.0000000005\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4900 \tLoss eqn: 0.0000302115 Loss data: 0.0000000028\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4910 \tLoss eqn: 0.0000299890 Loss data: 0.0000000078\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4920 \tLoss eqn: 0.0000298431 Loss data: 0.0000000024\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4930 \tLoss eqn: 0.0000296052 Loss data: 0.0000000047\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4940 \tLoss eqn: 0.0000294029 Loss data: 0.0000000044\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4950 \tLoss eqn: 0.0000291986 Loss data: 0.0000000037\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4960 \tLoss eqn: 0.0000294747 Loss data: 0.0000000210\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4970 \tLoss eqn: 0.0000287872 Loss data: 0.0000000060\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4980 \tLoss eqn: 0.0000308949 Loss data: 0.0000000544\n",
      "learning rate is  1.6000000000000004e-06\n",
      "Train Epoch: 4990 \tLoss eqn: 0.0000295753 Loss data: 0.0000000398\n",
      "learning rate is  1.6000000000000004e-06\n",
      "elapse time in parallel =  154.47548699378967\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#import foamFileOperation\n",
    "from matplotlib import pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "#from torchvision import datasets, transforms\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, TensorDataset,RandomSampler\n",
    "from math import exp, sqrt,pi\n",
    "import time\n",
    "import math\n",
    "\n",
    "# https://github.com/amir-cardiolab/PINN-wss/blob/main/1D-advection-diffusion/myPINN_1d_nwall2.py\n",
    "\n",
    "def geo_train(device,x_in,xb,cb,batchsize,learning_rate,epochs,path,Flag_batch,C_analytical,Vel,Diff):\n",
    "\tif (Flag_batch):\n",
    "\t\tx = torch.Tensor(x_in)\t\n",
    "\t\tdataset = TensorDataset(x,x)\n",
    "\t\tdataloader = DataLoader(dataset, batch_size=batchsize,shuffle=True,num_workers = 0,drop_last = False )\n",
    "\telse:\n",
    "\t\tx = torch.Tensor(x_in)  \n",
    "\t\th_n = 100 \n",
    "\t\tinput_n = 1 # this is what our answer is a function of. \n",
    "\t\tclass Swish(nn.Module):\n",
    "\t\t\tdef __init__(self, inplace=True):\n",
    "\t\t\t\tsuper(Swish, self).__init__()\n",
    "\t\t\t\tself.inplace = inplace\n",
    "\n",
    "\t\t\tdef forward(self, x):\n",
    "\t\t\t\tif self.inplace:\n",
    "\t\t\t\t\tx.mul_(torch.sigmoid(x))\n",
    "\t\t\t\t\treturn x\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn x * torch.sigmoid(x)\n",
    "\t\t\n",
    "\t\tclass Net2(nn.Module):\n",
    "\n",
    "\t\t\t#The __init__ function stack the layers of the \n",
    "\t\t\t#network Sequentially \n",
    "\t\t\tdef __init__(self):\n",
    "\t\t\t\tsuper(Net2, self).__init__()\n",
    "\t\t\t\tself.main = nn.Sequential(\n",
    "\t\t\t\t\tnn.Linear(input_n,h_n),\n",
    "\t\t\t\t\t#nn.Tanh(),\n",
    "\t\t\t\t\t#nn.Sigmoid(),\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\t\t\t\t\t#nn.Tanh(),\n",
    "\t\t\t\t\t#nn.Sigmoid(),\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\t\t\t\t\t#nn.Tanh(),\n",
    "\t\t\t\t\t#nn.Sigmoid(),\n",
    "\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\n",
    "\t\t\t\t\t#######\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\n",
    "\n",
    "\t\t\t\t\tnn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\tSwish(),\n",
    "\n",
    "\n",
    "\t\t\t\t\t#######\n",
    "\t\t\t\t\t#nn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\t#Swish(),\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t#nn.Linear(h_n,h_n),\n",
    "\n",
    "\t\t\t\t\t#Swish(),\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\t\tnn.Linear(h_n,1),\n",
    "\t\t\t\t)\n",
    "\t\t\t#This function defines the forward rule of\n",
    "\t\t\t#output respect to input.\n",
    "\t\t\tdef forward(self,x):\n",
    "\t\t\t\toutput = self.main(x)\n",
    "\t\t\t\tif (Flag_BC_near_wall):\n",
    "\t\t\t\t\t#output = output*x*(1-x) + (-0.9*x + 1.) #modify output to satisfy BC automatically #PINN-transfer-learning-BC-20\n",
    "\t\t\t\t\treturn  output * (1-x) + 0.1 #enforce BC at the wall region of interest\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn  output #* (x) + 1.0\n",
    "\t\t\t\t\t\n",
    "\t\n",
    "\t################################################################\n",
    "\n",
    "\tnet2 = Net2().to(device)\n",
    "\t\n",
    "\tdef init_normal(m):\n",
    "\t\tif type(m) == nn.Linear:\n",
    "\t\t\tnn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\t# use the modules apply function to recursively apply the initialization\n",
    "\tnet2.apply(init_normal)\n",
    "\n",
    "\n",
    "\t############################################################################\n",
    "\n",
    "\toptimizer2 = optim.Adam(net2.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\t\n",
    "\n",
    "\tdef criterion(x):\n",
    "\n",
    "\t\t#print (x)\n",
    "\t\t#x = torch.Tensor(x).to(device)\n",
    "\t\n",
    "\n",
    "\t\tx.requires_grad = True\n",
    "\t\t\n",
    "\n",
    "\t\tnet_in = x\n",
    "\t\tC = net2(net_in)\n",
    "\t\tC = C.view(len(C),-1)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t\tc_x = torch.autograd.grad(C,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "\t\tc_xx = torch.autograd.grad(c_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "\t\t\n",
    "\t\tloss_1 = Vel * c_x - Diff * c_xx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t# MSE LOSS\n",
    "\t\tloss_f = nn.MSELoss()\n",
    "\n",
    "\t\t#Note our target is zero. It is residual so we use zeros_like\n",
    "\t\tloss = loss_f(loss_1,torch.zeros_like(loss_1)) \n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\t###################################################################\n",
    "\tdef Loss_BC(xb,cb):\n",
    "\t\txb = torch.FloatTensor(xb).to(device)\n",
    "\t\tcb = torch.FloatTensor(cb).to(device)\n",
    "\n",
    "\t\t#xb.requires_grad = True\n",
    "\t\t\n",
    "\t\t#net_in = torch.cat((xb),1)\n",
    "\t\tout = net2(xb)\n",
    "\t\tcNN = out.view(len(out), -1)\n",
    "\t\t#cNN = cNN*(1.-xb) + cb    #cNN*xb*(1-xb) + cb\n",
    "\t\tloss_f = nn.MSELoss()\n",
    "\t\tloss_bc = loss_f(cNN, cb)\n",
    "\t\treturn loss_bc\n",
    "\n",
    "\t\t###################################################################\n",
    "\tdef Loss_data(xd,cd):\n",
    "\t\txd = torch.FloatTensor(xd).to(device)\n",
    "\t\tcd = torch.FloatTensor(cd).to(device)\n",
    "\n",
    "\t\t#xb.requires_grad = True\n",
    "\t\t\n",
    "\n",
    "\t\tout = net2(xd)\n",
    "\t\tout = out.view(len(out), -1)\n",
    "\t\tloss_f = nn.MSELoss()\n",
    "\t\tloss_bc = loss_f(out, cd)\n",
    "\n",
    "\t\treturn loss_bc\n",
    "\n",
    "\n",
    "\n",
    "\t# Main loop\n",
    "\tLOSS = []\n",
    "\ttic = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\txd = x_in[ [ int(nPt/2), int(3*nPt/4), int(9.8*nPt/10)    ] ] \n",
    "\tcd = C_analytical[ [ int(nPt/2), int(3*nPt/4), int(9.8*nPt/10)   ] ]\n",
    "\n",
    "\n",
    "\tif (Flag_schedule):\n",
    "\t\tscheduler_c = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=step_epoch, gamma=decay_rate)\n",
    "\n",
    "\n",
    "\tif(Flag_batch):# This one uses dataloader\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tfor batch_idx, (x_in2,x_in2) in enumerate(dataloader):\n",
    "\t\t\t\t#zero gradient\n",
    "\t\t\t\t#net1.zero_grad()\n",
    "\t\t\t\t##Closure function for LBFGS loop:\n",
    "\t\t\t\t#def closure():\n",
    "\t\t\t\tnet2.zero_grad()\n",
    "\t\t\t\t#x_in2 = torch.FloatTensor(x_in2)\n",
    "\t\t\t\t#print (x_in2)\n",
    "\t\t\t\t#print (x)\n",
    "\t\t\t\t#print('shape of x_in2',x_in2.shape)\n",
    "\t\t\t\tloss_eqn = criterion(x_in2)\n",
    "\t\t\t\tloss_data = Loss_data(xd,cd)\n",
    "\t\t\t\tloss = loss_eqn + Lambda_data * loss_data\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\n",
    "\t\t\t\toptimizer2.step() \n",
    "\t\t\tif (Flag_schedule):\n",
    "\t\t\t\t\tscheduler_c.step()\n",
    "\n",
    "\t\t\tif epoch % 10  ==0:\n",
    "\t\t\t\t\t#print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.10f}'.format(\n",
    "\t\t\t\t\t#\tepoch, batch_idx * len(x), len(dataloader.dataset),\n",
    "\t\t\t\t\t#\t100. * batch_idx / len(dataloader), loss.item()))\n",
    "\t\t\t\t\tprint('Train Epoch: {} \\tLoss eqn: {:.10f} Loss data: {:.10f}'.format(\n",
    "\t\t\t\t\tepoch, loss_eqn.item(), loss_data.item()))\n",
    "\t\t\t\t\tif (Flag_schedule):\n",
    "\t\t\t\t\t\tprint('learning rate is ', optimizer2.param_groups[0]['lr'])\n",
    "\t\t\t\t#if epoch %100 == 0:\n",
    "\t\t\t\t#\ttorch.save(net2.state_dict(),path+\"geo_para_axisy_sigma\"+str(sigma)+\"_epoch\"+str(epoch)+\"hard_u.pt\")\n",
    "\telse:\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\t#zero gradient\n",
    "\t\t\t#net1.zero_grad()\n",
    "\t\t\t##Closure function for LBFGS loop:\n",
    "\t\t\t#def closure():\n",
    "\t\t\tnet2.zero_grad()\n",
    "\t\t\tloss_eqn = criterion(x.to(device))\n",
    "\t\t\t#loss_bc = Loss_BC(xb,cb)\n",
    "\t\t\tloss_data = Loss_data(xd,cd)\n",
    "\t\t\tloss = loss_eqn + Lambda_data * loss_data\n",
    "\t\t\t\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer2.step() \n",
    "\t\t\t\n",
    "\t\t\tif (Flag_schedule):\n",
    "\t\t\t\tscheduler_c.step()\n",
    "\n",
    "\t\t\tif epoch % 10 ==0:\n",
    "\t\t\t\tprint('Train Epoch: {} \\tLoss eqn: {:.10f} Loss data: {:.10f}'.format(\n",
    "\t\t\t\t\tepoch, loss_eqn.item(), loss_data.item()))\n",
    "\t\t\t\tif (Flag_schedule):\n",
    "\t\t\t\t\tprint('learning rate is ', optimizer2.param_groups[0]['lr'])\n",
    "\t\t\n",
    "\n",
    "\ttoc = time.time()\n",
    "\telapseTime = toc - tic\n",
    "\tprint (\"elapse time in parallel = \", elapseTime)\n",
    "\t###################\n",
    "\t#plot\n",
    "\toutput = net2(x.to(device))  #evaluate model\n",
    "\tC_Result = output.cpu().data.numpy()\n",
    "\tplt.figure()\n",
    "\tplt.plot(x.cpu().detach().numpy(), C_analytical[:], '--', label='True solution', alpha=0.5) #analytical\n",
    "\tplt.plot(x.cpu().detach().numpy() , C_Result, 'go', label='PINN solution', alpha=0.5) #PINN\n",
    "\tplt.plot(xd[:], cd[:], 'r+', label='Sparse data', alpha=0.5) #data pts\n",
    "\tplt.legend(loc='best')\n",
    "\tplt.show()\n",
    "\n",
    "\treturn net2\n",
    "\n",
    "\t############################################################\n",
    "\t##save loss\n",
    "\t##myFile = open('Loss track'+'stenosis_para'+'.csv','w')#\n",
    "\t##with myFile:\n",
    "\t\t#writer = csv.writer(myFile)\n",
    "\t\t#writer.writerows(LOSS)\n",
    "\t#LOSS = np.array(LOSS)\n",
    "\t#np.savetxt('Loss_track_pipe_para.csv',LOSS)\n",
    "\n",
    "\t############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#Main code:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs  = 5000 \n",
    "\n",
    "Flag_batch =  False #USe batch or not  \n",
    "Flag_Chebyshev = False   #Use Chebyshev pts for more accurcy in BL region\n",
    "Flag_BC_near_wall =False  # True #If True sets BC at just the boundary of interet\n",
    "\n",
    "Lambda_data = 10.  #Data lambda\n",
    "\n",
    "Vel = 1.0\n",
    "Diff= 0.01 \n",
    "\n",
    "nPt = 100 \n",
    "xStart = 0.\n",
    "xEnd = 1.\n",
    "\n",
    "batchsize = 32 \n",
    "learning_rate = 1e-6 \n",
    "\n",
    "\n",
    "Flag_schedule = True  #If true change the learning rate \n",
    "if (Flag_schedule):\n",
    "\tlearning_rate = 1e-3  #starting learning rate\n",
    "\tstep_epoch = 1000 \n",
    "\tdecay_rate = 0.2  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if(Flag_Chebyshev): #!!!Not a very good idea (makes even the simpler case worse)\n",
    " x = np.polynomial.chebyshev.chebpts1(2*nPt)\n",
    " x = x[nPt:]\n",
    " if(0):#Mannually place more pts at the BL \n",
    "    x = np.linspace(0.95, xEnd, nPt)\n",
    "    x[1] = 0.2\n",
    "    x[2] = 0.5\n",
    " x[0] = 0.\n",
    " x[-1] = xEnd\n",
    " x = np.reshape(x, (nPt,1))\n",
    "else:\n",
    " x = np.linspace(xStart, xEnd, nPt)\n",
    " x = np.reshape(x, (nPt,1))\n",
    "\n",
    "\n",
    "print('shape of x',x.shape)\n",
    "\n",
    "#boundary pt and boundary condition\n",
    "#X_BC_loc = 1.\n",
    "#C_BC = 1.\n",
    "#xb = np.array([X_BC_loc],dtype=np.float32)\n",
    "#cb = np.array([C_BC ], dtype=np.float32)\n",
    "C_BC1 = 1.\n",
    "C_BC2 = 0.1\n",
    "xb = np.array([0.,1.],dtype=np.float32)\n",
    "cb = np.array([C_BC1,C_BC2 ], dtype=np.float32)\n",
    "xb= xb.reshape(-1, 1) #need to reshape to get 2D array\n",
    "cb= cb.reshape(-1, 1) #need to reshape to get 2D array\n",
    "#xb = np.transpose(xb)  #transpose because of the order that NN expects instances of training data\n",
    "#cb = np.transpose(cb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = \"Results/\"\n",
    "\n",
    "#Analytical soln\n",
    "A = (C_BC2 - C_BC1) / (exp(Vel/Diff) - 1)\n",
    "B = C_BC1 - A\n",
    "C_analytical = A*np.exp(Vel/Diff*x[:] ) + B\n",
    "\n",
    "\n",
    "\n",
    "#path = pre+\"aneurysmsigma01scalepara_100pt-tmp_\"+str(ii)\n",
    "net2_final = geo_train(device,x,xb,cb,batchsize,learning_rate,epochs,path,Flag_batch,C_analytical,Vel,Diff )\n",
    "#tic = time.time()\n",
    "\n",
    "#elapseTime = toc - tic\n",
    "#print (\"elapse time in serial = \", elapseTime)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53a9d46bace0d87d5d8b47eb975286de82fb882fc38f144f4a3850996fb362a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
