{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Model running via: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "dir_path = os.path.abspath(os.path.dirname(\"\"))\n",
    "NACA2412 = pd.read_excel(f\"{dir_path}/data/NACA2412 Cm over AoA.xlsx\").values # .values converts to numpy arrays\n",
    "NACA0012 = pd.read_excel(f\"{dir_path}/data/NACA0012 Cm over AoA.xlsx\").values\n",
    "# test = print(torch.from_numpy(NACA0012).shape)\n",
    "\n",
    "\n",
    "# NACA2412 will be the TARGET data that we want to predict which will be the source of HF points\n",
    "# whereas NACA0012 will be the LF data that will guide us in the MFNN\n",
    "\n",
    "\n",
    "# The model's aim will be to learn the relationship between AOA and CM (X = AOA, Y = CM)\n",
    "def tt_split(NACA2412, NACA0012):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_LF = NACA0012[:,0]\n",
    "    X_HF = NACA2412[:,0]\n",
    "    \n",
    "    Y_LF = NACA0012[:,1]\n",
    "    Y_HF = NACA2412[:,1]\n",
    "    \n",
    "    X_LF_TRAIN, X_LF_TEST, Y_LF_TRAIN, Y_LF_TEST = train_test_split(X_LF, Y_LF, test_size=0.1, shuffle=31)\n",
    "    X_HF_TRAIN, X_HF_TEST, Y_HF_TRAIN, Y_HF_TEST = train_test_split(X_HF, Y_HF, test_size=0.8, shuffle=31)\n",
    "\n",
    "    return X_LF_TRAIN, Y_LF_TRAIN, X_HF_TRAIN, Y_HF_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minmaxscaling():\n",
    "    def __init__(self, X_LF_TRAIN, Y_LF_TRAIN, X_HF_TRAIN, Y_HF_TRAIN):\n",
    "        import numpy as np\n",
    "\n",
    "        self.x_lf = X_LF_TRAIN\n",
    "        self.y_lf = Y_LF_TRAIN\n",
    "        self.x_hf = X_HF_TRAIN\n",
    "        self.y_hf = Y_HF_TRAIN\n",
    "        xmin = min(min(self.x_lf), min(self.x_hf))\n",
    "        xmax = max(max(self.x_lf), max(self.x_hf))\n",
    "        self.x = np.linspace(xmin, xmax, 1001)[:, np.newaxis]\n",
    "\n",
    "    def prep(self):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        import numpy as np\n",
    "\n",
    "        self.x_lf = self.x_lf.reshape(-1, 1)\n",
    "        self.x_hf = self.x_hf.reshape(-1, 1)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(self.x)\n",
    "        self.x_lf = scaler.transform(self.x_lf)\n",
    "        self.x_hf = scaler.transform(self.x_hf)\n",
    "        self.x = scaler.transform(self.x)\n",
    "        \n",
    "        if len(np.shape(self.y_lf)) == 1:\n",
    "            self.y_lf = self.y_lf.reshape(-1, 1)\n",
    "            self.y_hf = self.y_hf.reshape(-1, 1)\n",
    "            \n",
    "        datascaler = MinMaxScaler()\n",
    "        datascaler.fit(self.y_lf)\n",
    "        self.y_lf = datascaler.transform(self.y_lf)\n",
    "        self.y_hf = datascaler.transform(self.y_hf)\n",
    "        \n",
    "        return self.x, self.x_lf, self.y_lf, self.x_hf, self.y_hf\n",
    "    \n",
    "X_LF_TRAIN, Y_LF_TRAIN, X_HF_TRAIN, Y_HF_TRAIN = tt_split(NACA2412, NACA0012)\n",
    "\n",
    "scaled_data = minmaxscaling(X_LF_TRAIN, Y_LF_TRAIN, X_HF_TRAIN, Y_HF_TRAIN).prep()\n",
    "X, X_LF_TRAIN, Y_LF_TRAIN, X_HF_TRAIN, Y_HF_TRAIN = scaled_data\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(X_HF_TRAIN, Y_HF_TRAIN)\n",
    "# plt.scatter(X_LF_TRAIN, Y_LF_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globally Shared Model(s) Parameters\n",
    "lf_num_epochs = 600\n",
    "hf_num_epochs = 600\n",
    "MF_epochs = 600\n",
    "\n",
    "hidden_dims = ([100, 100, 100, 100, 100, 100, 100, 100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low Fidelity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_input_dim = 2\n",
    "LF_output_dim = 2\n",
    "\n",
    "class LowFidelityNetwork(torch.nn.Module):\n",
    "  def __init__(self, hidden_dims, LF_input_dim, LF_output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(LF_input_dim, hidden_dims[0])\n",
    "    # self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "    self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "    self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "    self.fc4 = nn.Linear(hidden_dims[2], hidden_dims[3])\n",
    "    self.fc5 = nn.Linear(hidden_dims[3], hidden_dims[4])\n",
    "    self.fc6 = nn.Linear(hidden_dims[4], hidden_dims[5])\n",
    "    self.fc7 = nn.Linear(hidden_dims[5], hidden_dims[6])\n",
    "    self.fcEND = nn.Linear(hidden_dims[6], LF_output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    skip_connection = x\n",
    "    # x = self.bn1(x)\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    x = torch.relu(self.fc5(x))\n",
    "    x = torch.relu(self.fc6(x))\n",
    "    x = torch.relu(self.fc7(x))\n",
    "    x = x + skip_connection\n",
    "    x = self.fcEND(x)\n",
    "    return x\n",
    "\n",
    "LF_model = LowFidelityNetwork(hidden_dims, LF_input_dim, LF_output_dim).to(device)\n",
    "\n",
    "for param in LF_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_model.train()\n",
    "\n",
    "# Training\n",
    "LF_losses = [] \n",
    "val_losses = []\n",
    "prev_loss = []\n",
    "LF_loss = torch.zeros(1)\n",
    "\n",
    "LF_batch_size = 175\n",
    "\n",
    "# LF_loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(LF_model.parameters(), lr=0.0001) # weight_decay=1e-5\n",
    "\n",
    "\n",
    "for epoch in range(lf_num_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(X_LF_train.size()[0])\n",
    "    \n",
    "    for i in range(0,X_LF_train.size()[0], LF_batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+LF_batch_size]\n",
    "        batch_x, batch_y = X_LF_train[indices], Y_LF_train[indices]\n",
    "    \n",
    "        prev_loss = LF_loss.item()\n",
    "        outputs = LF_model.forward(batch_x)\n",
    "        LF_loss = criterion(outputs,batch_y)\n",
    "        LF_losses.append(LF_loss.item())\n",
    "        LF_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 600 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{hf_num_epochs}], Loss: {LF_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(LF_losses, label = \"Training\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LF_Loss')\n",
    "plt.title('Low Fidelity Training Loss Graph of Model')\n",
    "\n",
    "plt.plot(val_losses, \"--\" , label = \"Testing\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Low Fidelity Validation Loss Graph of Model')\n",
    "plt.grid(which='both', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "# plt.show()\n",
    "\n",
    "print(f\"Epochs needed (out of {lf_num_epochs}): {len(LF_losses)}\")\n",
    "print(f\"LF Training Loss: {LF_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Fidelity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_input_dim = 2\n",
    "HF_output_dim = 2\n",
    "\n",
    "class HighFidelityNetwork(torch.nn.Module):\n",
    "  def __init__(self, hidden_dims, HF_input_dim, HF_output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(HF_input_dim, hidden_dims[0])\n",
    "    # self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "    self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "    self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "    self.fc4 = nn.Linear(hidden_dims[2], hidden_dims[3])\n",
    "    self.fc5 = nn.Linear(hidden_dims[3], hidden_dims[4])\n",
    "    self.fc6 = nn.Linear(hidden_dims[4], hidden_dims[5])\n",
    "    self.fc7 = nn.Linear(hidden_dims[5], hidden_dims[6])\n",
    "    self.fcEND = nn.Linear(hidden_dims[7], HF_output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    # x = self.bn1(x)\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    x = torch.relu(self.fc5(x))\n",
    "    x = torch.relu(self.fc6(x))\n",
    "    x = torch.relu(self.fc7(x))\n",
    "    x = self.fcEND(x)\n",
    "    return x\n",
    "\n",
    "HF_model = HighFidelityNetwork(hidden_dims, HF_input_dim, HF_output_dim).to(device)\n",
    "\n",
    "for param in HF_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_model.train()\n",
    "\n",
    "# Training\n",
    "HF_losses = [] \n",
    "val_losses = []\n",
    "prev_loss = []\n",
    "HF_loss = torch.zeros(1)\n",
    "\n",
    "HF_batch_size = 45\n",
    "\n",
    "# HF_loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(HF_model.parameters(), lr=0.00008, weight_decay=1e-6) # weight_decay=1e-5\n",
    "\n",
    "for epoch in range(hf_num_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(X_HF_train.size()[0])\n",
    "    \n",
    "    for i in range(0,X_HF_train.size()[0], HF_batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+HF_batch_size]\n",
    "        batch_x, batch_y = X_HF_train[indices], Y_HF_train[indices]\n",
    "    \n",
    "        prev_loss = HF_loss.item()\n",
    "        outputs = HF_model.forward(batch_x)\n",
    "        HF_loss = criterion(outputs,batch_y)\n",
    "        HF_losses.append(HF_loss.item())\n",
    "        HF_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 600 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{hf_num_epochs}], Loss: {HF_loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(HF_losses, label = \"Training\")\n",
    "\n",
    "plt.plot(val_losses, \"--\" , label = \"Testing\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LF Losses')\n",
    "plt.title('High Fidelity Training and Validation Losses of Model')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(which='both', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# print(f\"Error: {HF_average_percentage_error}\")\n",
    "print(f\"Epochs needed (out of {hf_num_epochs}): {len(HF_losses)}\")\n",
    "print(f\"HF Training Loss: {HF_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Fidelity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1mean = LF_model(X_HF_TRAIN.to(device))\n",
    "\n",
    "L2train = torch.hstack((X_HF_TRAIN, L1mean)) # think of the house price example (sqr feet, rooms, garden, etc.)\n",
    "print(L2train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_input_dim = 3\n",
    "MF_output_dim = 2\n",
    "\n",
    "class MultiFidelityNetwork(torch.nn.Module):\n",
    "  def __init__(self, hidden_dims, MF_input_dim, MF_output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(MF_input_dim, hidden_dims[0])\n",
    "    \n",
    "    self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "    self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "    self.fc4 = nn.Linear(hidden_dims[2], hidden_dims[3])\n",
    "    self.fc5 = nn.Linear(hidden_dims[3], hidden_dims[4])\n",
    "    self.fc6 = nn.Linear(hidden_dims[4], hidden_dims[5])\n",
    "    self.fc7 = nn.Linear(hidden_dims[5], hidden_dims[6])\n",
    "    self.fcEND = nn.Linear(hidden_dims[6], MF_output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    x = torch.relu(self.fc5(x))\n",
    "    x = torch.relu(self.fc6(x))\n",
    "    x = torch.relu(self.fc7(x))\n",
    "    x = self.fcEND(x)\n",
    "    return x\n",
    "\n",
    "MF_model = MultiFidelityNetwork(hidden_dims, MF_input_dim, MF_output_dim).to(device)\n",
    "\n",
    "for param in MF_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_model.train()\n",
    "\n",
    "# Training\n",
    "MF_losses = [] \n",
    "val_losses = []\n",
    "prev_loss = []\n",
    "MF_loss = torch.zeros(1)\n",
    "\n",
    "# MF_loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(MF_model.parameters(), lr=0.0001) # weight_decay=1e-5\n",
    "\n",
    "for epoch in range(MF_epochs):\n",
    "\n",
    "    # Training\n",
    "    prev_loss = MF_loss.item()\n",
    "    MF_y_pred = MF_model(L2train)\n",
    "    MF_loss = criterion(MF_y_pred, Y_HF_TRAIN)\n",
    "    MF_losses.append(MF_loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    MF_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 300 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{MF_epochs}], Loss: {MF_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(MF_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MF Loss')\n",
    "plt.title('Multi-Fidelity Training Loss of Model')\n",
    "plt.grid(which='both', alpha=0.5)\n",
    "print(f\"MF Training Loss: {MF_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models\n",
    "LF_model.eval()\n",
    "HF_model.eval()\n",
    "MF_model.eval()\n",
    "\n",
    "# Define the NEW input data\n",
    "with torch.no_grad(): \n",
    "    y_LF_pred = LF_model(X.to(device))\n",
    "    y_HF_pred = HF_model(X.to(device)) \n",
    "    \n",
    "    # Step 4: Add X and output of the LF model (similar to Step 2)\n",
    "    L2test = torch.hstack((X.to(device), y_LF_pred))\n",
    "    y_MF_pred = MF_model(L2test.to(device)) \n",
    "    \n",
    "\n",
    "xL, yL = torch.split(y_LF_pred, [1, 1], dim=1)\n",
    "xH, yH = torch.split(y_HF_pred, [1, 1], dim=1)\n",
    "xMF, yMF = torch.split(y_MF_pred, [1, 1], dim=1)\n",
    "\n",
    "\n",
    "# ###\n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.plot(xL.cpu().detach().numpy(), yL.cpu().detach().numpy())\n",
    "# plt.plot(xH.cpu().detach().numpy(), yH.cpu().detach().numpy())\n",
    "# plt.plot(xMF.cpu().detach().numpy(), yMF.cpu().detach().numpy())\n",
    "# # plt.scatter(xH, yH)\n",
    "###\n",
    "\n",
    "\n",
    "linewidth = 3\n",
    "lw = 2\n",
    "\n",
    "# Target LÄ°ne\n",
    "Z_X, Z_Y = torch.split(HF(Z), [1, 1], dim=1)\n",
    "\n",
    "# Low Fidelity Points and Prediction\n",
    "x_lf_train, y_lf_train = torch.split(Y_LF_train, [1, 1], dim=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5.5))\n",
    "ax.plot(Z_X, Z_Y, \"k\", label= \"Target \", linewidth=linewidth)\n",
    "ax.plot(xL.cpu().detach().numpy(), yL.cpu().detach().numpy(),\"--r\", lw=lw, label= \"LF Prediction \",)\n",
    "ax.plot(x_lf_train.cpu().detach().numpy(), y_lf_train.cpu().detach().numpy(), \"o\", color=(0.6350, 0.0780, 0.1840), markersize = 5, label = 'LF Data Points')\n",
    "\n",
    "# ax.set_title('LOW Fidelity Network Predictions', fontsize=13)\n",
    "ax.grid(which='both', alpha=0.2, linestyle = \"--\")\n",
    "plt.tight_layout()\n",
    "ax.set_xticklabels([]); ax.set_yticklabels([]); ax.set_xticks([]); ax.set_yticks([])\n",
    "plt.savefig(\"LF_Plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# High Fidelity Points and Prediction\n",
    "x_hf_train, y_hf_train = torch.split(Y_HF_train, [1, 1], dim=1)\n",
    "ax.plot(Z_X, Z_Y, \"k\", label= \"Target \", linewidth=linewidth)\n",
    "ax.plot(xH.cpu().detach().numpy(), yH.cpu().detach().numpy(),'--b', lw=lw, label= \"HF Prediction \")\n",
    "ax.plot(x_hf_train.cpu().detach().numpy(), y_hf_train.cpu().detach().numpy(),\"bo\" , markersize = 7.5, label = 'HF Data Points')\n",
    "\n",
    "# ax.set_title('HIGH Fidelity Network Predictions', fontsize=13)\n",
    "ax.grid(which='both', alpha=0.2, linestyle = \"--\")\n",
    "plt.tight_layout()\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.savefig(\"HF_Plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5.5))\n",
    "ax.plot(Z_X, Z_Y, \"k\", label= \"Target \", linewidth=2.5)\n",
    "ax.plot(xMF.cpu().detach().numpy(), yMF.cpu().detach().numpy(),'--',color=(1, 0, 1), lw=3, label= \"MF Prediction \")\n",
    "plt.tight_layout()\n",
    "ax.set_xticklabels([]); ax.set_yticklabels([]); ax.set_xticks([]); ax.set_yticks([]); \n",
    "plt.savefig(\"MF_Plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#! Offset data guides the HF data points to form an accurate MF prediction\n",
    "#! Like testing a scaled model in a wind tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Point Distribution\n",
    "# fig, ax = plt.subplots(figsize=(10,3))\n",
    "# ax.plot(Z, HF(Z), label= \"Target \")\n",
    "# ax.grid(which='both', alpha=0.5)\n",
    "# ax.set_xlabel('X', fontsize=11)\n",
    "# ax.set_ylabel('Y', fontsize=11)\n",
    "# ax.set_title('Low Fidelity Network Predictions', fontsize=16)\n",
    "# ax.plot(Z.cpu().detach().numpy(), y_LF_pred.cpu().detach().numpy(),'--k', lw=2, label= \"LF Prediction \")\n",
    "# ax.plot(X_LF_train.cpu().detach().numpy(), Y_LF_train.cpu().detach().numpy(), 'ro', markersize = 3, label = 'LF Data Points')\n",
    "\n",
    "# ax.legend(loc='upper left', ncol=1, fontsize='small')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,3))\n",
    "# ax.plot(Z, HF(Z), label= \"Target \")\n",
    "# ax.plot(Z.cpu().detach().numpy(), y_HF_pred.cpu().detach().numpy(),'--k', lw=2, label= \"HF Prediction \")\n",
    "# ax.plot(X_HF_train.cpu().detach().numpy(), Y_HF_train.cpu().detach().numpy(), 'bo', markersize = 4, label = 'HF Data Points')\n",
    "# ax.grid(which='both', alpha=0.5)\n",
    "# ax.set_xlabel('X', fontsize=11)\n",
    "# ax.set_ylabel('Y', fontsize=11)\n",
    "# ax.legend(loc='upper left', ncol=1, fontsize='small')\n",
    "\n",
    "# ax.set_title('High Fidelity Network Predictions', fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,3))\n",
    "# ax.plot(Z, HF(Z), label= \"Target\")\n",
    "# ax.plot(Z.cpu().detach().numpy(), y_MF_pred.cpu().detach().numpy(),'k', lw=2, label= \"MF Prediction \")\n",
    "# ax.grid(which='both', alpha=0.5)\n",
    "# ax.set_xlabel('X', fontsize=11)\n",
    "# ax.set_ylabel('Y', fontsize=11)\n",
    "# ax.set_title('Multi Fidelity Network Prediction', fontsize=16)\n",
    "# ax.legend(loc='upper left', ncol=1, fontsize='small')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Saving Numpy Values\n",
    "# np.savetxt(\"data/Z.txt\", Z)\n",
    "# np.savetxt(\"data/HF_Z.txt\", HF(Z))\n",
    "# np.savetxt(\"data/LF_Z.txt\", LF(Z))\n",
    "\n",
    "# np.savetxt(\"data/LF_PRED.txt\", y_LF_pred.cpu().detach().numpy())\n",
    "# np.savetxt(\"data/HF_PRED.txt\", y_HF_pred.cpu().detach().numpy())\n",
    "# np.savetxt(\"data/MF_PRED.txt\", y_MF_pred.cpu().detach().numpy())\n",
    "\n",
    "# np.savetxt(\"data/Y_LF_TRAIN.txt\", Y_LF_train.cpu().detach().numpy())\n",
    "# np.savetxt(\"data/Y_HF_TRAIN.txt\", Y_HF_train.cpu().detach().numpy())\n",
    "# np.savetxt(\"data/X_HF_TRAIN.txt\", X_HF_train.cpu().detach().numpy())\n",
    "# np.savetxt(\"data/X_LF_TRAIN.txt\", X_LF_train.cpu().detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53a9d46bace0d87d5d8b47eb975286de82fb882fc38f144f4a3850996fb362a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
