{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mQH_a\u001b[39m(y):\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39m6\u001b[39m \u001b[39m*\u001b[39m y \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msin(\u001b[39m12\u001b[39m \u001b[39m*\u001b[39m y \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m x_lf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m2430\u001b[39m, \u001b[39m7940\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m     50\u001b[0m x_hf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m8000\u001b[39m, \u001b[39m3512\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m     52\u001b[0m lf \u001b[39m=\u001b[39m QL_a(x_lf)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "class MFRegress:\n",
    "    def __init__(self, x_lf, lf, x_hf, hf,\n",
    "                 embedding_theory=True,\n",
    "                 gradient=False):\n",
    "        import numpy as np\n",
    "\n",
    "        self.x_lf = x_lf\n",
    "        self.lf = lf\n",
    "        self.x_hf = x_hf\n",
    "        self.hf = hf\n",
    "        self.embedding_theory = embedding_theory\n",
    "        self.gradient = gradient\n",
    "        xmin = min(min(self.x_lf), min(self.x_hf))\n",
    "        xmax = max(max(self.x_lf), max(self.x_hf))\n",
    "        self.x = np.linspace(xmin, xmax, 1001)[:, np.newaxis]\n",
    "\n",
    "    def prep(self):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        import numpy as np\n",
    "\n",
    "        self.x_lf = self.x_lf.reshape(-1, 1)\n",
    "        self.x_hf = self.x_hf.reshape(-1, 1)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(self.x)\n",
    "        self.x_lf = scaler.transform(self.x_lf)\n",
    "        self.x_hf = scaler.transform(self.x_hf)\n",
    "        self.x = scaler.transform(self.x)\n",
    "        \n",
    "        if len(np.shape(self.lf)) == 1:\n",
    "            self.lf = self.lf.reshape(-1, 1)\n",
    "            self.hf = self.hf.reshape(-1, 1)\n",
    "        print(self.lf)\n",
    "        datascaler = MinMaxScaler()\n",
    "        datascaler.fit(self.lf)\n",
    "        self.lf = datascaler.transform(self.lf)\n",
    "        self.hf = datascaler.transform(self.hf)\n",
    "        print(self.x_lf)\n",
    "        \n",
    "        return x_lf, lf, x_hf, hf\n",
    "    \n",
    "    \n",
    "def QL_a(y):\n",
    "    return 0.5 * (6 * y - 2) ** 2 * np.sin(12 * y - 4) + 10 * (y - 0.5) - 5\n",
    "\n",
    "def QH_a(y):\n",
    "    return (6 * y - 2)**2 * np.sin(12 * y - 4)\n",
    "\n",
    "\n",
    "x_lf = np.linspace(-2430, 7940, 10)\n",
    "x_hf = np.linspace(-8000, 3512, 10)\n",
    "\n",
    "lf = QL_a(x_lf)\n",
    "hf = QH_a(x_hf)    \n",
    "\n",
    "annen = MFRegress(x_lf, lf, x_hf, hf).prep()\n",
    "x_lf, lf, x_hf, hf = annen\n",
    "\n",
    "print(lf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_lf = np.linspace(-2430, 7940, 100)\n",
    "x_hf = np.linspace(-8000, 3512, 100)\n",
    "\n",
    "##\n",
    "xmin = min(min(x_lf), min(x_hf)) # grabs the minimum and max values of both fidelities for the X range \n",
    "xmax = max(max(x_lf), max(x_hf)) # compares both and takes the highest of either max of max... easy\n",
    "X = np.linspace(xmin, xmax, 1001)[:, None]\n",
    "##\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X) # fits the data to X and then .transform transforms it dependind on X, you can transform lf and hf data this way\n",
    "\n",
    "X_scaled = torch.from_numpy(scaler.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscale the data\n",
    "X_unscaled = torch.from_numpy(scaler.inverse_transform(X_scaled))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are building a regression model and you want to make predictions on the original scale, you'll need to unscale the predictions.\n",
    "\n",
    "On the other hand, if you are using the scaled data for some other purpose such as training a model, you may not need to unscale it. The choice ultimately depends on your specific use case and the desired output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53a9d46bace0d87d5d8b47eb975286de82fb882fc38f144f4a3850996fb362a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
