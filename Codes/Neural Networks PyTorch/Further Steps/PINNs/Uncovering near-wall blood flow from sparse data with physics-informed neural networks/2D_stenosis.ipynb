{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /scratch/aa3878/PINN/stenosis/PINN/sten_mesh000000.vtu\n",
      "n_points of the mesh: 0\n",
      "shape of x (0, 1)\n",
      "shape of y (0, 1)\n",
      "Loading /scratch/aa3878/PINN/stenosis/PINN/inlet_BC.vtk\n",
      "n_points of at inlet 0\n",
      "Loading /scratch/aa3878/PINN/stenosis/PINN/wall_BC.vtk\n",
      "n_points of at wall 0\n",
      "shape of xb (0, 1)\n",
      "shape of yb (0, 1)\n",
      "shape of ub (0, 1)\n",
      "Loading /scratch/aa3878/PINN/stenosis/PINN/velocity_sten_steady.vtu\n",
      "n_points of the data file read: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'GetDataType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 851\u001b[0m\n\u001b[0;32m    849\u001b[0m probe\u001b[39m.\u001b[39mUpdate()\n\u001b[0;32m    850\u001b[0m array \u001b[39m=\u001b[39m probe\u001b[39m.\u001b[39mGetOutput()\u001b[39m.\u001b[39mGetPointData()\u001b[39m.\u001b[39mGetArray(fieldname)\n\u001b[1;32m--> 851\u001b[0m data_vel \u001b[39m=\u001b[39m VN\u001b[39m.\u001b[39;49mvtk_to_numpy(array)\n\u001b[0;32m    855\u001b[0m data_vel_u \u001b[39m=\u001b[39m data_vel[:,\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m U_scale\n\u001b[0;32m    856\u001b[0m data_vel_v \u001b[39m=\u001b[39m data_vel[:,\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m U_scale\n",
      "File \u001b[1;32mc:\\Users\\emjan\\AppData\\Local\\VisualStudioCode\\Dissertation_Venv\\dis_env\\lib\\site-packages\\vtkmodules\\util\\numpy_support.py:215\u001b[0m, in \u001b[0;36mvtk_to_numpy\u001b[1;34m(vtk_array)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvtk_to_numpy\u001b[39m(vtk_array):\n\u001b[0;32m    201\u001b[0m     \u001b[39m\"\"\"Converts a VTK data array to a numpy array.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[39m    Given a subclass of vtkDataArray, this function returns an\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     typ \u001b[39m=\u001b[39m vtk_array\u001b[39m.\u001b[39;49mGetDataType()\n\u001b[0;32m    216\u001b[0m     \u001b[39massert\u001b[39;00m typ \u001b[39min\u001b[39;00m get_vtk_to_numpy_typemap()\u001b[39m.\u001b[39mkeys(), \\\n\u001b[0;32m    217\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mUnsupported array type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39mtyp\n\u001b[0;32m    218\u001b[0m     \u001b[39massert\u001b[39;00m typ \u001b[39m!=\u001b[39m vtkConstants\u001b[39m.\u001b[39mVTK_BIT, \u001b[39m'\u001b[39m\u001b[39mBit arrays are not supported.\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'GetDataType'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#import foamFileOperation\n",
    "from matplotlib import pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "#from torchvision import datasets, transforms\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, TensorDataset,RandomSampler\n",
    "from math import exp, sqrt,pi\n",
    "import time\n",
    "import vtk\n",
    "from vtkmodules.util import numpy_support as VN\n",
    "#import torch.optim.lr_scheduler.StepLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def geo_train(device,x_in,y_in,xb,yb,ub,vb,xd,yd,ud,vd,batchsize,learning_rate,epochs,path,Flag_batch,Diff,rho,Flag_BC_exact,Lambda_BC,nPt,T,xb_inlet,yb_inlet,ub_inlet,vb_inlet ):\n",
    "\tif (Flag_batch):\n",
    "            x = torch.Tensor(x_in).to(device)\n",
    "            y = torch.Tensor(y_in).to(device)\n",
    "            #dataset = TensorDataset(x,y)\n",
    "            xb = torch.Tensor(xb).to(device)\n",
    "            yb = torch.Tensor(yb).to(device)\n",
    "            ub = torch.Tensor(ub).to(device)\n",
    "            vb = torch.Tensor(vb).to(device)\n",
    "            xd = torch.Tensor(xd).to(device)\n",
    "            yd = torch.Tensor(yd).to(device)\n",
    "            ud = torch.Tensor(ud).to(device)\n",
    "            vd = torch.Tensor(vd).to(device)\n",
    "            #dist = torch.Tensor(dist).to(device)\n",
    "            xb_inlet = torch.Tensor(xb_inlet).to(device)\n",
    "            yb_inlet = torch.Tensor(yb_inlet).to(device)\n",
    "            ub_inlet = torch.Tensor(ub_inlet).to(device)\n",
    "            vb_inlet = torch.Tensor(vb_inlet).to(device)\n",
    "            if(1): #Cuda slower in double? \n",
    "                x = x.type(torch.cuda.FloatTensor)\n",
    "                y = y.type(torch.cuda.FloatTensor)\n",
    "                xb = xb.type(torch.cuda.FloatTensor)\n",
    "                yb = yb.type(torch.cuda.FloatTensor)\n",
    "                ub = ub.type(torch.cuda.FloatTensor)\n",
    "                vb = vb.type(torch.cuda.FloatTensor)\n",
    "                #dist = dist.type(torch.cuda.FloatTensor)\n",
    "                xb_inlet = xb_inlet.type(torch.cuda.FloatTensor)\n",
    "                yb_inlet = yb_inlet.type(torch.cuda.FloatTensor)\n",
    "                ub_inlet = ub_inlet.type(torch.cuda.FloatTensor)\n",
    "                vb_inlet = vb_inlet.type(torch.cuda.FloatTensor)\n",
    "                xd = xd.type(torch.cuda.FloatTensor)\n",
    "                yd = yd.type(torch.cuda.FloatTensor)\n",
    "                ud = ud.type(torch.cuda.FloatTensor)\n",
    "                vd = vd.type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "                dataset = TensorDataset(x,y)\n",
    "                #dataset_bc = TensorDataset(x,y,xb,yb,ub,vb,dist)\n",
    "\n",
    "                #dataloader = DataLoader(dataset, batch_size=batchsize,shuffle=True,num_workers = 0,drop_last = False )\n",
    "                dataloader = DataLoader(dataset, batch_size=batchsize,shuffle=True,num_workers = 0, drop_last = True )\n",
    "                #dataloader_bc = DataLoader(dataset_bc, batch_size=batchsize,shuffle=True,num_workers = 0, drop_last = False )\n",
    "            else:\n",
    "                x = torch.Tensor(x_in).to(device)\n",
    "                y = torch.Tensor(y_in).to(device) \n",
    "                #t = torch.Tensor(t_in).to(device) \n",
    "                #x_test =  torch.Tensor(x_test).to(device)\n",
    "                #y_test  = torch.Tensor(y_test).to(device)  \n",
    "\n",
    "                h_n = 128 #Width for u,v,p\n",
    "                input_n = 2 # this is what our answer is a function of (x,y)\n",
    "                class Swish(nn.Module):\n",
    "                    def __init__(self, inplace=True):\n",
    "                        super(Swish, self).__init__()\n",
    "                        self.inplace = inplace\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        if self.inplace:\n",
    "                            x.mul_(torch.sigmoid(x))\n",
    "                            return x\n",
    "                        else:\n",
    "                            return x * torch.sigmoid(x)\n",
    "                class MySquared(nn.Module):\n",
    "                    def __init__(self, inplace=True):\n",
    "                        super(MySquared, self).__init__()\n",
    "                        self.inplace = inplace\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        return torch.square(x)\n",
    "\n",
    "                \n",
    "\n",
    "                class Net2_u(nn.Module):\n",
    "\n",
    "                    #The __init__ function stack the layers of the \n",
    "                    #network Sequentially \n",
    "                    def __init__(self):\n",
    "                        super(Net2_u, self).__init__()\n",
    "                        self.main = nn.Sequential(\n",
    "                            nn.Linear(input_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "\n",
    "                            Swish(),\n",
    "\n",
    "                            nn.Linear(h_n,1),\n",
    "                        )\n",
    "                    #This function defines the forward rule of\n",
    "                    #output respect to input.\n",
    "                    #def forward(self,x):\n",
    "                    def forward(self,x):\t\n",
    "                        output = self.main(x)\n",
    "                        #output_bc = net1_bc_u(x)\n",
    "                        #output_dist = net1_dist(x)\n",
    "                        if (Flag_BC_exact):\n",
    "                            output = output*(x- xStart) *(y- yStart) * (y- yEnd ) + U_BC_in + (y- yStart) * (y- yEnd )  #modify output to satisfy BC automatically #PINN-transfer-learning-BC-20\n",
    "                        #return  output * (y_in-yStart) * (y_in- yStart_up)\n",
    "                        #return output * dist_bc + v_bc\n",
    "                        #return output *output_dist * Dist_net_scale + output_bc\n",
    "                        return output\n",
    "\n",
    "                class Net2_v(nn.Module):\n",
    "\n",
    "                    #The __init__ function stack the layers of the \n",
    "                    #network Sequentially \n",
    "                    def __init__(self):\n",
    "                        super(Net2_v, self).__init__()\n",
    "                        self.main = nn.Sequential(\n",
    "                            nn.Linear(input_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "\n",
    "                            nn.Linear(h_n,1),\n",
    "                        )\n",
    "                    #This function defines the forward rule of\n",
    "                    #output respect to input.\n",
    "                    #def forward(self,x):\n",
    "                    def forward(self,x):\t\n",
    "                        output = self.main(x)\n",
    "                        #output_bc = net1_bc_v(x)\n",
    "                        #output_dist = net1_dist(x)\n",
    "                        if (Flag_BC_exact):\n",
    "                            output = output*(x- xStart) *(x- xEnd )*(y- yStart) *(y- yEnd ) + (-0.9*x + 1.) #modify output to satisfy BC automatically #PINN-transfer-learning-BC-20\n",
    "                        #return  output * (y_in-yStart) * (y_in- yStart_up)\n",
    "                        #return output * dist_bc + v_bc\n",
    "                        #return output *output_dist * Dist_net_scale + output_bc\n",
    "                        return output\n",
    "\n",
    "                class Net2_p(nn.Module):\n",
    "\n",
    "                    #The __init__ function stack the layers of the \n",
    "                    #network Sequentially \n",
    "                    def __init__(self):\n",
    "                        super(Net2_p, self).__init__()\n",
    "                        self.main = nn.Sequential(\n",
    "                            nn.Linear(input_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "                            #nn.Tanh(),\n",
    "                            #nn.Sigmoid(),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "                            nn.Linear(h_n,h_n),\n",
    "\n",
    "                            #nn.BatchNorm1d(h_n),\n",
    "\n",
    "                            Swish(),\n",
    "\n",
    "                            nn.Linear(h_n,1),\n",
    "                        )\n",
    "                    #This function defines the forward rule of\n",
    "                    #output respect to input.\n",
    "                    def forward(self,x):\n",
    "                        output = self.main(x)\n",
    "                        #print('shape of xnet',x.shape) #Resuklts: shape of xnet torch.Size([batchsize, 2]) \n",
    "                        if (Flag_BC_exact):\n",
    "                            output = output*(x- xStart) *(x- xEnd )*(y- yStart) *(y- yEnd ) + (-0.9*x + 1.) #modify output to satisfy BC automatically #PINN-transfer-learning-BC-20\n",
    "                        #return  (1-x[:,0]) * output[:,0]  #Enforce P=0 at x=1 #Shape of output torch.Size([batchsize, 1])\n",
    "                        return  output\n",
    "                \n",
    "                ################################################################\n",
    "                #net1 = Net1().to(device)\n",
    "                net2_u = Net2_u().to(device)\n",
    "                net2_v = Net2_v().to(device)\n",
    "                net2_p = Net2_p().to(device)\n",
    "\n",
    "                \n",
    "                def init_normal(m):\n",
    "                    if type(m) == nn.Linear:\n",
    "                        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "                # use the modules apply function to recursively apply the initialization\n",
    "                net2_u.apply(init_normal)\n",
    "                net2_v.apply(init_normal)\n",
    "                net2_p.apply(init_normal)\n",
    "\n",
    "\n",
    "                ############################################################################\n",
    "\n",
    "                optimizer_u = optim.Adam(net2_u.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "                optimizer_v = optim.Adam(net2_v.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "                optimizer_p = optim.Adam(net2_p.parameters(), lr=learning_rate, betas = (0.9,0.99),eps = 10**-15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                def criterion(x,y):\n",
    "\n",
    "                    #print (x)\n",
    "                    #x = torch.Tensor(x).to(device)\n",
    "                    #y = torch.Tensor(y).to(device)\n",
    "                    #t = torch.Tensor(t).to(device)\n",
    "\n",
    "                    #x = torch.FloatTensor(x).to(device)\n",
    "                    #x= torch.from_numpy(x).to(device)\n",
    "\n",
    "                    x.requires_grad = True\n",
    "                    y.requires_grad = True\n",
    "                    #t.requires_grad = True\n",
    "                    #u0 = u0.detach()\n",
    "                    #v0 = v0.detach()\n",
    "                    \n",
    "                    #net_in = torch.cat((x),1)\n",
    "                    net_in = torch.cat((x,y),1)\n",
    "                    u = net2_u(net_in)\n",
    "                    u = u.view(len(u),-1)\n",
    "                    v = net2_v(net_in)\n",
    "                    v = v.view(len(v),-1)\n",
    "                    P = net2_p(net_in)\n",
    "                    P = P.view(len(P),-1)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    u_x = torch.autograd.grad(u,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "                    u_xx = torch.autograd.grad(u_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "                    u_y = torch.autograd.grad(u,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "                    u_yy = torch.autograd.grad(u_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "                    v_x = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "                    v_xx = torch.autograd.grad(v_x,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "                    v_y = torch.autograd.grad(v,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "                    v_yy = torch.autograd.grad(v_y,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "\n",
    "                    P_x = torch.autograd.grad(P,x,grad_outputs=torch.ones_like(x),create_graph = True,only_inputs=True)[0]\n",
    "                    P_y = torch.autograd.grad(P,y,grad_outputs=torch.ones_like(y),create_graph = True,only_inputs=True)[0]\n",
    "\n",
    "                    #u_t = torch.autograd.grad(u,t,grad_outputs=torch.ones_like(t),create_graph = True,only_inputs=True)[0]\n",
    "                    #v_t = torch.autograd.grad(v,t,grad_outputs=torch.ones_like(t),create_graph = True,only_inputs=True)[0]\n",
    "                    \n",
    "                    XX_scale = U_scale * (X_scale**2)\n",
    "                    YY_scale = U_scale * (Y_scale**2)\n",
    "                    UU_scale  = U_scale **2\n",
    "                \n",
    "                    loss_2 = u*u_x / X_scale + v*u_y / Y_scale - Diff*( u_xx/XX_scale  + u_yy /YY_scale  )+ 1/rho* (P_x / (X_scale*UU_scale)   )  #X-dir\n",
    "                    loss_1 = u*v_x / X_scale + v*v_y / Y_scale - Diff*( v_xx/ XX_scale + v_yy / YY_scale )+ 1/rho*(P_y / (Y_scale*UU_scale)   ) #Y-dir\n",
    "                    loss_3 = (u_x / X_scale + v_y / Y_scale) #continuity\n",
    "\n",
    "\n",
    "\n",
    "                    # MSE LOSS\n",
    "                    loss_f = nn.MSELoss()\n",
    "\n",
    "                    #Note our target is zero. It is residual so we use zeros_like\n",
    "                    loss = loss_f(loss_1,torch.zeros_like(loss_1))+  loss_f(loss_2,torch.zeros_like(loss_2))+  loss_f(loss_3,torch.zeros_like(loss_3))\n",
    "\n",
    "                    return loss\n",
    "\n",
    "                def Loss_BC(xb,yb,ub,vb, xb_inlet, yb_inlet, ub_inlet, x, y ):\n",
    "\n",
    "                    \n",
    "\n",
    "                    net_in1 = torch.cat((xb, yb), 1)\n",
    "                    out1_u = net2_u(net_in1)\n",
    "                    out1_v = net2_v(net_in1)\n",
    "                    \n",
    "                    out1_u = out1_u.view(len(out1_u), -1)\n",
    "                    out1_v = out1_v.view(len(out1_v), -1)\n",
    "\n",
    "                    #net_in2 = torch.cat((xb_inlet, yb_inlet), 1)\n",
    "                    #out2_u = net2_u(net_in2)\n",
    "                    #out2_v = net2_v(net_in2)\n",
    "                    \n",
    "                    #out2_u = out2_u.view(len(out2_u), -1)\n",
    "                    #out2_v = out2_v.view(len(out2_v), -1)\n",
    "\n",
    "                \n",
    "\n",
    "                    loss_f = nn.MSELoss()\n",
    "                    loss_noslip = loss_f(out1_u, torch.zeros_like(out1_u)) + loss_f(out1_v, torch.zeros_like(out1_v)) \n",
    "                    #loss_inlet = loss_f(out2_u, ub_inlet) + loss_f(out2_v, torch.zeros_like(out2_v) )\n",
    "\n",
    "                    #return 1.* loss_noslip + loss_inlet\n",
    "                    return loss_noslip\n",
    "\n",
    "\n",
    "                def Loss_data(xd,yd,ud,vd ):\n",
    "                \n",
    "\n",
    "                    #xb.requires_grad = True\n",
    "                    #xd.requires_grad = True\n",
    "                    #yd.requires_grad = True\n",
    "                    \n",
    "                    #net_in = torch.cat((xb),1)\n",
    "                    net_in1 = torch.cat((xd, yd), 1)\n",
    "                    out1_u = net2_u(net_in1)\n",
    "                    out1_v = net2_v(net_in1)\n",
    "                    \n",
    "                    out1_u = out1_u.view(len(out1_u), -1)\n",
    "                    out1_v = out1_v.view(len(out1_v), -1)\n",
    "\n",
    "                \n",
    "\n",
    "                    loss_f = nn.MSELoss()\n",
    "                    loss_d = loss_f(out1_u, ud) + loss_f(out1_v, vd) \n",
    "\n",
    "\n",
    "                    return loss_d\n",
    "\n",
    "                # Main loop\n",
    "\n",
    "                tic = time.time()\n",
    "\n",
    "\n",
    "                if(Flag_pretrain):\n",
    "                    print('Reading (pretrain) functions first...')\n",
    "                    net2_u.load_state_dict(torch.load(path+\"sten_u\" + \".pt\"))\n",
    "                    net2_v.load_state_dict(torch.load(path+\"sten_v\" + \".pt\"))\n",
    "                    net2_p.load_state_dict(torch.load(path+\"sten_p\" + \".pt\"))\n",
    "                \n",
    "                    \n",
    "\n",
    "                if (Flag_schedule):\n",
    "                    scheduler_u = torch.optim.lr_scheduler.StepLR(optimizer_u, step_size=step_epoch, gamma=decay_rate)\n",
    "                    scheduler_v = torch.optim.lr_scheduler.StepLR(optimizer_v, step_size=step_epoch, gamma=decay_rate)\n",
    "                    scheduler_p = torch.optim.lr_scheduler.StepLR(optimizer_p, step_size=step_epoch, gamma=decay_rate)\n",
    "\n",
    "                if(Flag_batch):# This one uses dataloader\n",
    "                        \n",
    "                        for epoch in range(epochs):\n",
    "                            #for batch_idx, (x_in,y_in) in enumerate(dataloader):  \n",
    "                            #for batch_idx, (x_in,y_in,xb_in,yb_in,ub_in,vb_in) in enumerate(dataloader): \n",
    "                            loss_eqn_tot = 0.\n",
    "                            loss_bc_tot = 0.\n",
    "                            loss_data_tot = 0.\n",
    "                            n = 0\n",
    "                            for batch_idx, (x_in,y_in) in enumerate(dataloader): \n",
    "                                #net_in = torch.cat((x_in,y_in),1)\n",
    "                                #u_bc = net1_bc_u(net_in)\n",
    "                                #v_bc = net1_bc_v(net_in)\n",
    "                                #dist_bc = net1_dist(net_in)\n",
    "\n",
    "                \n",
    "                                net2_u.zero_grad()\n",
    "                                net2_v.zero_grad()\n",
    "                                net2_p.zero_grad()\n",
    "                                loss_eqn = criterion(x_in,y_in)\n",
    "                                loss_bc = Loss_BC(xb,yb,ub,vb,xb_inlet,yb_inlet,ub_inlet,x,y)\n",
    "                                loss_data = Loss_data(xd,yd,ud,vd)\n",
    "                                loss = loss_eqn + Lambda_BC* loss_bc + Lambda_data*loss_data\n",
    "                                loss.backward()\n",
    "                                optimizer_u.step() \n",
    "                                optimizer_v.step()\n",
    "                                #optimizer_psi.step()  \n",
    "                                optimizer_p.step()  \n",
    "                                loss_eqn_tot += loss_eqn\n",
    "                                loss_bc_tot += loss_bc\n",
    "                                loss_data_tot  += loss_data\n",
    "                                n += 1 \n",
    "                                if batch_idx % 40 ==0:\n",
    "                                    #loss_bc = Loss_BC(xb,yb,ub,vb) #causes out of memory issue for large data in cuda\n",
    "                                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss eqn {:.10f} Loss BC {:.8f} Loss data {:.8f}'.format(\n",
    "                                        epoch, batch_idx * len(x_in), len(dataloader.dataset),\n",
    "                                        100. * batch_idx / len(dataloader), loss_eqn.item(), loss_bc.item(),loss_data.item()))\n",
    "                                    #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss eqn {:.10f} '.format(\n",
    "                                    #\tepoch, batch_idx * len(x_in), len(dataloader.dataset),\n",
    "                                    #\t100. * batch_idx / len(dataloader), loss.item()))\n",
    "                            if (Flag_schedule):\n",
    "                                    scheduler_u.step()\n",
    "                                    scheduler_v.step()\n",
    "                                    scheduler_p.step()\n",
    "                            loss_eqn_tot = loss_eqn_tot / n\n",
    "                            loss_bc_tot = loss_bc_tot / n\n",
    "                            loss_data_tot = loss_data_tot / n\n",
    "                            print('*****Total avg Loss : Loss eqn {:.10f} Loss BC {:.10f} Loss data {:.10f} ****'.format(loss_eqn_tot, loss_bc_tot,loss_data_tot) )\n",
    "                            print('learning rate is ', optimizer_u.param_groups[0]['lr'], optimizer_v.param_groups[0]['lr'])\n",
    "                    \n",
    "                        if(0): #This causes out of memory in cuda in autodiff\n",
    "                            loss_eqn = criterion(x,y)\t\n",
    "                            loss_bc = Loss_BC(xb,yb,ub,vb)\n",
    "                            loss = loss_eqn #+ Lambda_BC* loss_bc\n",
    "                            print('**** Final (all batches) \\tLoss: {:.10f} \\t Loss BC {:.6f}'.format(\n",
    "                                loss.item(),loss_bc.item()))\n",
    "\n",
    "                else:\n",
    "                    for epoch in range(epochs):\n",
    "                        #zero gradient\n",
    "                        #net1.zero_grad()\n",
    "                        ##Closure function for LBFGS loop:\n",
    "                        #def closure():\n",
    "                        net2.zero_grad()\n",
    "                        loss_eqn = criterion(x,y)\n",
    "                        loss_bc = Loss_BC(xb,yb,cb)\n",
    "                        if (Flag_BC_exact):\n",
    "                            loss = loss_eqn #+ loss_bc\n",
    "                        else:\n",
    "                            loss = loss_eqn + Lambda_BC * loss_bc\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer_u.step() \n",
    "                        optimizer_v.step() \n",
    "                        optimizer_p.step() \n",
    "                        if epoch % 10 ==0:\n",
    "                            print('Train Epoch: {} \\tLoss: {:.10f} \\t Loss BC {:.6f}'.format(\n",
    "                                epoch, loss.item(),loss_bc.item()))\n",
    "\n",
    "                toc = time.time()\n",
    "                elapseTime = toc - tic\n",
    "                print (\"elapse time in parallel = \", elapseTime)\n",
    "                ###################\n",
    "                #plot\n",
    "                if(1):#save network\n",
    "                    torch.save(net2_p.state_dict(),path+\"sten_data_p\" + \".pt\")\n",
    "                    torch.save(net2_u.state_dict(),path+\"sten_data_u\" + \".pt\")\n",
    "                    torch.save(net2_v.state_dict(),path+\"sten_data_v\" + \".pt\")\n",
    "\n",
    "                    print (\"Data saved!\")\n",
    "\n",
    "\n",
    "                \n",
    "                net_in = torch.cat((x.requires_grad_(),y.requires_grad_()),1)\n",
    "                output_u = net2_u(net_in)  #evaluate model (runs out of memory for large GPU problems!)\n",
    "                output_v = net2_v(net_in)  #evaluate model\n",
    "\n",
    "\n",
    "                output_u = output_u.cpu().data.numpy() #need to convert to cpu before converting to numpy\n",
    "                output_v = output_v.cpu().data.numpy()\n",
    "                x = x.cpu()\n",
    "                y = y.cpu()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                plt.figure()\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.scatter(x.detach().numpy(), y.detach().numpy(), c = output_u , cmap = 'rainbow')\n",
    "                plt.title('NN results, u')\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "                plt.figure()\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.scatter(x.detach().numpy(), y.detach().numpy(), c = output_v , cmap = 'rainbow')\n",
    "                plt.title('NN results, v')\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                return \n",
    "\n",
    "                ############################################################\n",
    "                ##save loss\n",
    "                ##myFile = open('Loss track'+'stenosis_para'+'.csv','w')#\n",
    "                ##with myFile:\n",
    "                    #writer = csv.writer(myFile)\n",
    "                    #writer.writerows(LOSS)\n",
    "                #LOSS = np.array(LOSS)\n",
    "                #np.savetxt('Loss_track_pipe_para.csv',LOSS)\n",
    "\n",
    "                ############################################################\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#Main code:\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "Flag_batch = True \n",
    "Flag_BC_exact = False \n",
    "Lambda_BC  = 20.\n",
    "\n",
    "Lambda_data = 1.\n",
    "\n",
    "#Directory = \"/home/aa3878/Data/ML/Amir/stenosis/\"\n",
    "Directory = \"/scratch/aa3878/PINN/stenosis/PINN/\"\n",
    "mesh_file = Directory + \"sten_mesh000000.vtu\"\n",
    "bc_file_in = Directory + \"inlet_BC.vtk\"\n",
    "bc_file_wall = Directory + \"wall_BC.vtk\"\n",
    "\n",
    "File_data = Directory + \"velocity_sten_steady.vtu\"\n",
    "fieldname = 'f_5-0' #The velocity field name in the vtk file (see from ParaView)\n",
    "\n",
    "batchsize = 256 \n",
    "learning_rate = 1e-5 \n",
    "\n",
    "\n",
    "epochs  = 5500 \n",
    "\n",
    "Flag_pretrain = False # True #If true reads the nets from last run\n",
    "\n",
    "\n",
    "Diff = 0.001\n",
    "rho = 1.\n",
    "T = 0.5 #total duraction\n",
    "#nPt_time = 50 #number of time-steps\n",
    "\n",
    "Flag_x_length = True #if True scales the eqn such that the length of the domain is = X_scale\n",
    "X_scale = 2.0 #The length of the  domain (need longer length for separation region)\n",
    "Y_scale = 1.0 \n",
    "U_scale = 1.0\n",
    "U_BC_in = 0.5\n",
    "\n",
    "\n",
    "Lambda_div = 1.  #penalty factor for continuity eqn (Makes it worse!?)\n",
    "Lambda_v = 1.  #penalty factor for y-momentum equation\n",
    "\n",
    "#https://stackoverflow.com/questions/60050586/pytorch-change-the-learning-rate-based-on-number-of-epochs\n",
    "Flag_schedule = True #If true change the learning rate \n",
    "if (Flag_schedule):\n",
    "    learning_rate = 5e-4 #starting learning rate\n",
    "    step_epoch = 1200 #1000\n",
    "    decay_rate = 0.1 # 0.1\n",
    "\n",
    "\n",
    "if (not Flag_x_length):\n",
    "    X_scale = 1.\n",
    "    Y_scale = 1.\n",
    "\n",
    "\n",
    "print ('Loading', mesh_file)\n",
    "reader = vtk.vtkXMLUnstructuredGridReader()\n",
    "reader.SetFileName(mesh_file)\n",
    "reader.Update()\n",
    "data_vtk = reader.GetOutput()\n",
    "n_points = data_vtk.GetNumberOfPoints()\n",
    "print ('n_points of the mesh:' ,n_points)\n",
    "x_vtk_mesh = np.zeros((n_points,1))\n",
    "y_vtk_mesh = np.zeros((n_points,1))\n",
    "VTKpoints = vtk.vtkPoints()\n",
    "for i in range(n_points):\n",
    "\tpt_iso  =  data_vtk.GetPoint(i)\n",
    "\tx_vtk_mesh[i] = pt_iso[0]\t\n",
    "\ty_vtk_mesh[i] = pt_iso[1]\n",
    "\tVTKpoints.InsertPoint(i, pt_iso[0], pt_iso[1], pt_iso[2])\n",
    "\n",
    "point_data = vtk.vtkUnstructuredGrid()\n",
    "point_data.SetPoints(VTKpoints)\n",
    "\n",
    "x  = np.reshape(x_vtk_mesh , (np.size(x_vtk_mesh [:]),1)) \n",
    "y  = np.reshape(y_vtk_mesh , (np.size(y_vtk_mesh [:]),1))\n",
    "\n",
    "\n",
    "\n",
    "nPt = 130  \n",
    "xStart = 0.\n",
    "xEnd = 1.\n",
    "yStart = 0.\n",
    "yEnd = 1.0\n",
    "delta_circ = 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t = np.linspace(0., T, nPt*nPt)\n",
    "t=t.reshape(-1, 1)\n",
    "print('shape of x',x.shape)\n",
    "print('shape of y',y.shape)\n",
    "#print('shape of t',t.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Define boundary points\n",
    "print ('Loading', bc_file_in)\n",
    "reader = vtk.vtkUnstructuredGridReader()\n",
    "reader.SetFileName(bc_file_in)\n",
    "reader.Update()\n",
    "data_vtk = reader.GetOutput()\n",
    "n_points = data_vtk.GetNumberOfPoints()\n",
    "print ('n_points of at inlet' ,n_points)\n",
    "x_vtk_mesh = np.zeros((n_points,1))\n",
    "y_vtk_mesh = np.zeros((n_points,1))\n",
    "VTKpoints = vtk.vtkPoints()\n",
    "for i in range(n_points):\n",
    "\tpt_iso  =  data_vtk.GetPoint(i)\n",
    "\tx_vtk_mesh[i] = pt_iso[0]\t\n",
    "\ty_vtk_mesh[i] = pt_iso[1]\n",
    "\tVTKpoints.InsertPoint(i, pt_iso[0], pt_iso[1], pt_iso[2])\n",
    "point_data = vtk.vtkUnstructuredGrid()\n",
    "point_data.SetPoints(VTKpoints)\n",
    "xb_in  = np.reshape(x_vtk_mesh , (np.size(x_vtk_mesh[:]),1)) \n",
    "yb_in  = np.reshape(y_vtk_mesh , (np.size(y_vtk_mesh[:]),1))\n",
    "\n",
    "print ('Loading', bc_file_wall)\n",
    "reader = vtk.vtkUnstructuredGridReader()\n",
    "reader.SetFileName(bc_file_wall)\n",
    "reader.Update()\n",
    "data_vtk = reader.GetOutput()\n",
    "n_pointsw = data_vtk.GetNumberOfPoints()\n",
    "print ('n_points of at wall' ,n_pointsw)\n",
    "x_vtk_mesh = np.zeros((n_pointsw,1))\n",
    "y_vtk_mesh = np.zeros((n_pointsw,1))\n",
    "VTKpoints = vtk.vtkPoints()\n",
    "for i in range(n_pointsw):\n",
    "\tpt_iso  =  data_vtk.GetPoint(i)\n",
    "\tx_vtk_mesh[i] = pt_iso[0]\t\n",
    "\ty_vtk_mesh[i] = pt_iso[1]\n",
    "\tVTKpoints.InsertPoint(i, pt_iso[0], pt_iso[1], pt_iso[2])\n",
    "point_data = vtk.vtkUnstructuredGrid()\n",
    "point_data.SetPoints(VTKpoints)\n",
    "xb_wall  = np.reshape(x_vtk_mesh , (np.size(x_vtk_mesh [:]),1)) \n",
    "yb_wall  = np.reshape(y_vtk_mesh , (np.size(y_vtk_mesh [:]),1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#u_in_BC = np.linspace(U_BC_in, U_BC_in, n_points) #constant uniform BC\n",
    "u_in_BC = (yb_in[:]) * ( 0.3 - yb_in[:] )  / 0.0225 * U_BC_in #parabolic\n",
    "\n",
    "\n",
    "v_in_BC = np.linspace(0., 0., n_points)\n",
    "u_wall_BC = np.linspace(0., 0., n_pointsw)\n",
    "v_wall_BC = np.linspace(0., 0., n_pointsw)\n",
    "#t_BC = np.linspace(0., T, nPt_BC)\n",
    "#t_BC = np.linspace(0., T, nPt_time)\n",
    "\n",
    "#t_BC = np.linspace(0., T, nPt_BC)\n",
    "#t_BC = np.linspace(0., T, nPt_time)\n",
    "\n",
    "#tb = np.concatenate((t_BC, t_BC, t_BC), 0)\n",
    "#xb = np.concatenate((xb_wall), 0)\n",
    "#yb = np.concatenate((yb_wall), 0)\n",
    "xb = xb_wall\n",
    "yb = yb_wall\n",
    "\n",
    "#ub = np.concatenate((u_wall_BC), 0)\n",
    "#vb = np.concatenate((v_wall_BC), 0)\n",
    "ub = u_wall_BC\n",
    "vb = v_wall_BC\n",
    "\n",
    "#xb_inlet = np.concatenate((xb_in), 0)\n",
    "#yb_inlet = np.concatenate((yb_in), 0)\n",
    "#ub_inlet = np.concatenate((u_in_BC), 0)\n",
    "#vb_inlet = np.concatenate((v_in_BC), 0)\n",
    "\n",
    "xb_inlet = xb_in \n",
    "yb_inlet =yb_in \n",
    "\n",
    "\n",
    "ub_inlet = u_in_BC\n",
    "vb_inlet = v_in_BC\n",
    "\n",
    "\n",
    "### Trying to set distance function with Dirichlet BC everywhere\n",
    "#xb_dist = np.concatenate((xleft, xup,xrightw, xdown,xdown2,xright), 0)\n",
    "#yb_dist = np.concatenate((yleft, yup,yrightw, ydown,ydown2,yright), 0)\n",
    "####\n",
    "\n",
    "\n",
    "#tb= tb.reshape(-1, 1) #need to reshape to get 2D array\n",
    "xb= xb.reshape(-1, 1) #need to reshape to get 2D array\n",
    "yb= yb.reshape(-1, 1) #need to reshape to get 2D array\n",
    "ub= ub.reshape(-1, 1) #need to reshape to get 2D array\n",
    "vb= vb.reshape(-1, 1) #need to reshape to get 2D array\n",
    "xb_inlet= xb_inlet.reshape(-1, 1) #need to reshape to get 2D array\n",
    "yb_inlet= yb_inlet.reshape(-1, 1) #need to reshape to get 2D array\n",
    "ub_inlet= ub_inlet.reshape(-1, 1) #need to reshape to get 2D array\n",
    "vb_inlet= vb_inlet.reshape(-1, 1) #need to reshape to get 2D array\n",
    "\n",
    "print('shape of xb',xb.shape)\n",
    "print('shape of yb',yb.shape)\n",
    "print('shape of ub',ub.shape)\n",
    "\n",
    "\n",
    "#V_IC = 0. #I.C. for all velocoties.\n",
    "#t_IC = np.linspace(0., 0., nPt*nPt)\n",
    "#u_IC = np.linspace(V_IC, V_IC, nPt*nPt)\n",
    "#v_IC = np.linspace(V_IC, V_IC, nPt*nPt)\n",
    "#t_IC= t_IC.reshape(-1, 1)\n",
    "#u_IC= u_IC.reshape(-1, 1)\n",
    "#v_IC= v_IC.reshape(-1, 1)\n",
    "\n",
    "\n",
    "path = \"Results/\"\n",
    "\n",
    "\n",
    "\n",
    "##### Read data here#########\n",
    "\n",
    "#!!specify pts location here:\n",
    "x_data = [1., 1.2, 1.22, 1.31, 1.39 ] \n",
    "y_data =[0.15, 0.07, 0.22, 0.036, 0.26 ]\n",
    "z_data  = [0.,0.,0.,0.,0. ]\n",
    "\n",
    "x_data = np.asarray(x_data)  #convert to numpy \n",
    "y_data = np.asarray(y_data) #convert to numpy \n",
    "\n",
    "\n",
    "print ('Loading', File_data)\n",
    "reader = vtk.vtkXMLUnstructuredGridReader()\n",
    "reader.SetFileName(File_data)\n",
    "reader.Update()\n",
    "data_vtk = reader.GetOutput()\n",
    "n_points = data_vtk.GetNumberOfPoints()\n",
    "print ('n_points of the data file read:' ,n_points)\n",
    "\n",
    "\n",
    "VTKpoints = vtk.vtkPoints()\n",
    "for i in range(len(x_data)): \n",
    "\tVTKpoints.InsertPoint(i, x_data[i] , y_data[i]  , z_data[i])\n",
    "\n",
    "point_data = vtk.vtkUnstructuredGrid()\n",
    "point_data.SetPoints(VTKpoints)\n",
    "\n",
    "probe = vtk.vtkProbeFilter()\n",
    "probe.SetInputData(point_data)\n",
    "probe.SetSourceData(data_vtk)\n",
    "probe.Update()\n",
    "array = probe.GetOutput().GetPointData().GetArray(fieldname)\n",
    "data_vel = VN.vtk_to_numpy(array)\n",
    "\n",
    "\n",
    "\n",
    "data_vel_u = data_vel[:,0] / U_scale\n",
    "data_vel_v = data_vel[:,1] / U_scale\n",
    "x_data = x_data / X_scale\n",
    "y_data = y_data / Y_scale\n",
    "\n",
    "print('Using input data pts: pts: ',x_data, y_data)\n",
    "print('Using input data pts: vel u: ',data_vel_u)\n",
    "print('Using input data pts: vel v: ',data_vel_v)\n",
    "xd= x_data.reshape(-1, 1) #need to reshape to get 2D array\n",
    "yd= y_data.reshape(-1, 1) #need to reshape to get 2D array\n",
    "ud= data_vel_u.reshape(-1, 1) #need to reshape to get 2D array\n",
    "vd= data_vel_v.reshape(-1, 1) #need to reshape to get 2D array\n",
    "\n",
    "\n",
    "#path = pre+\"aneurysmsigma01scalepara_100pt-tmp_\"+str(ii)\n",
    "geo_train(device,x,y,xb,yb,ub,vb,xd,yd,ud,vd,batchsize,learning_rate,epochs,path,Flag_batch,Diff,rho,Flag_BC_exact,Lambda_BC,nPt,T,xb_inlet,yb_inlet,ub_inlet,vb_inlet )\n",
    "#tic = time.time()\n",
    "\n",
    "#elapseTime = toc - tic\n",
    "#print (\"elapse time in serial = \", elapseTime)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53a9d46bace0d87d5d8b47eb975286de82fb882fc38f144f4a3850996fb362a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
