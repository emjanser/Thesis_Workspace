{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# randomizes the datapoint inputs every run\n",
    "np.random.seed(2) \n",
    "# np.random.seed(rnd.randint(0, 1000000)) # deactivated the randomizer for simplicity during testing\n",
    "\n",
    "# timer module to calculate run time\n",
    "time_start = time.time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: Setting up HF and LF data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 1000 points in [0,1] inclusive regularly spaced\n",
    "X = torch.linspace(0, 1, 1000)\n",
    "\n",
    "# defines the HF and LF sine functions. Gaussian noise was excluded for simplicity during testing\n",
    "def hf(x): \n",
    "    return 1.8*torch.sin(x * (8 * math.pi))*2*x # + torch.randn(x.size()) * math.sqrt(0.04) #* the accurately simulated sin model for HF data\n",
    "\n",
    "def lf(x):\n",
    "    return torch.sin(x * (8 * math.pi))*x # + torch.randn(x.size()) * math.sqrt(0.04) #* the inaccurately simulated sin model for LF data\n",
    "\n",
    "# Setting the number of LF and HF points\n",
    "Nlf=20\n",
    "Nhf=8\n",
    "\n",
    "#Sampling LF and HF model randomly\n",
    "X_lf = np.random.permutation(X)[0:Nlf] # basically randomly aranges the 50 different values in X array, from 0 to \"len(Nlf)\"\n",
    "X_hf = np.random.permutation(X_lf)[0:Nhf]\n",
    "\n",
    "# Converting np.ndarray to tensor to be able to utilize GPyTorch\n",
    "X_lf = torch.from_numpy(X_lf)\n",
    "X_hf = torch.from_numpy(X_hf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: HF Modelling using Exact Gaussian Process Model\n",
    "Step 1: Setting up the HF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling train_x and train_y data\n",
    "train_x = X_hf # Randomly arranged HF points\n",
    "train_y = hf(X_hf) # HF points ran through HF sine function (accurate sine function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Setting up the HF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest form of Gaussian processs model, Exact intereference\n",
    "class ExactGPModel(gpytorch.models.ExactGP): # gpytorch.models.ExactGP is the simplest GP Model.\n",
    "    def __init__(self, train_x, train_y, likelihood): \n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood) # Adds the training sets and likelihood to the GP model\n",
    "        self.mean_module = gpytorch.means.ConstantMean() #  A Mean - This defines the prior mean of the GP.\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) # A Kernel - This defines the prior covariance of the GP-\n",
    "        # a method of using a linear classifier to solve a non-linear problem.\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) # This is the object used to represent multivariate normal distributions.\n",
    "        # multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional normal distribution to higher dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Initializing the model and likelihood and smoke testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a number of training iterations for both models \n",
    "training_iter = 300\n",
    "\n",
    "# initialize likelihood and model \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood() #? how likekly a model has a certain parameter value given some data\n",
    "model = ExactGPModel(train_x, train_y, likelihood) # calling ExactGPModel from the class as \"model\"\n",
    "# print(model)\n",
    "\n",
    "# this is for running the notebook in our testing framework with an added smoke test \n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else training_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward() # Computes the gradient and stores them in the tensors once you call .backward() \n",
    "    \n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(): # turns off gradients computation\n",
    "    test_x = torch.linspace(0, 1, 1000)\n",
    "    observed_pred = likelihood(model(test_x)) # output of LF model with the likelihoodand model\n",
    "print(observed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: LF Model Modelling using Exact Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the low fidelity data values\n",
    "\n",
    "train_x2 = X_lf # Randomly arranged HF points\n",
    "train_y2 = lf(X_lf)  # LF points ran through LF sine function (inaccurate sine function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest form of Gaussian processs model, Exact intereference\n",
    "class ExactGPModel_LF(gpytorch.models.ExactGP): \n",
    "    def __init__(self, train_x2, train_y2, likelihood): \n",
    "        super(ExactGPModel, self).__init__(train_x2, train_y2, likelihood) \n",
    "        self.mean_module = gpytorch.means.ConstantMean() \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood() \n",
    "model_2 = ExactGPModel(train_x2, train_y2, likelihood) \n",
    "\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else training_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model and likelihood\n",
    "model_2.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=0.1) \n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model_2)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model_2(train_x2)\n",
    "\n",
    "    loss = -mll(output, train_y2)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x2 = torch.linspace(0, 1, 1000)\n",
    "    observed_pred2 = likelihood(model_2(test_x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: MF Model Stage \n",
    "I might need this code box below later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module1 = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module2 = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        # We learn an IndexKernel for 2 tasks\n",
    "        # (so we'll actually learn 2x2=4 tasks with correlations)\n",
    "        self.task_covar_module1 = TwoFidelityIndexKernel(num_tasks=2, rank=1)\n",
    "        self.task_covar_module2 = TwoFidelityIndexKernel(num_tasks=2, rank=1, includeParams=False)\n",
    "        print(f\"Initial value : Covar 1, lengthscale {self.covar_module1.base_kernel.lengthscale.item()}, prefactor {self.covar_module1.outputscale.item()}\")\n",
    "        print(f\"Initial value : Covar 2, lengthscale {self.covar_module2.base_kernel.lengthscale.item()}, prefactor {self.covar_module2.outputscale.item()}\")\n",
    "        \n",
    "    def forward(self,x,i):\n",
    "        mean_x = self.mean_module(x)\n",
    "        \n",
    "        # Get input-input covariance\n",
    "        covar1_x = self.covar_module1(x)\n",
    "        # Get task-task covariance\n",
    "        covar1_i = self.task_covar_module1(i)\n",
    "        \n",
    "        # Get input-input covariance\n",
    "        covar2_x = self.covar_module2(x)\n",
    "        # Get task-task covariance\n",
    "        covar2_i = self.task_covar_module2(i)\n",
    "        \n",
    "            \n",
    "        # Multiply the two together to get the covariance we want\n",
    "        covar1 = covar1_x.mul(covar1_i)\n",
    "        covar2 = covar2_x.mul(covar2_i)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar1+covar2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total failure here since best I can do is try to mix X_hf and L_hf inputs within the model\n",
    "# instead of using the LF model and predict using HF inputs\n",
    "# then using those values to add into MF model\n",
    "\n",
    "train_x3 = X_lf\n",
    "train_y3 = lf(X_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP): \n",
    "    def __init__(self, hf_p, train_3, likelihood): \n",
    "        super(ExactGPModel, self).__init__(hf_p, train_y3, likelihood) \n",
    "        self.mean_module = gpytorch.means.ConstantMean() \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 300\n",
    "\n",
    "hf_p = X_hf\n",
    "hf_p = torch.linspace(min(hf_p), max(hf_p), 20)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood() \n",
    "model_3 = ExactGPModel(hf_p, train_y3, likelihood)\n",
    "\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else training_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f, ax = plt.subplots(3, figsize = (10, 5))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower,  upper  =  observed_pred.confidence_region()\n",
    "    lower2, upper2 = observed_pred2.confidence_region()\n",
    "    lower3, upper3 = observed_pred3.confidence_region()\n",
    "    \n",
    "    #High Fidelity / Exact Line\n",
    "    ax[0].plot(X,hf(X), label=\"High Fidelity / Exact\")\n",
    "    ax[1].plot(X,hf(X), label=\"High Fidelity / Exact\")\n",
    "    ax[2].plot(X,hf(X), label=\"High Fidelity / Exact\")\n",
    "\n",
    "    # Sample Points\n",
    "    ax[0].plot(train_x.numpy(),train_y.numpy(), 'ro',label=\"High Fidelity Samples\")\n",
    "    ax[1].plot(train_x2.numpy(),train_y2.numpy(), 'bo', label=\"Low Fidelity Samples\")\n",
    "    \n",
    "    # GP Mean PREDICTIONS\n",
    "    ax[0].plot(test_x.numpy(),observed_pred.mean.numpy(), 'k', label = \"HF GP Mean (Trained on Red Dots)\")\n",
    "    ax[1].plot(test_x2.numpy(),observed_pred2.mean.numpy(), 'k', label = \"LF GP Mean (Train on Blue Dots)\")\n",
    "    ax[2].plot(test_x3.numpy(),observed_pred3.mean.numpy(), 'k', label = \"Deep GP Mean (Train on Blue Dots)\")\n",
    "    \n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax[0].fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.2, label = \"Confidence\")\n",
    "    ax[1].fill_between(test_x2.numpy(), lower2.numpy(), upper2.numpy(), alpha=0.2, label = \"Confidence\") \n",
    "    ax[2].fill_between(test_x3.numpy(), lower3.numpy(), upper3.numpy(), alpha=0.2, label = \"Confidence\") \n",
    "\n",
    "    #Legends\n",
    "    ax[2].legend(bbox_to_anchor=(0.9, 1), loc='upper left', fontsize='x-small')\n",
    "    ax[1].legend(bbox_to_anchor=(0.9, 1), loc='upper left', fontsize='x-small')\n",
    "    ax[0].legend(bbox_to_anchor=(0.9, 1), loc='upper left', fontsize='x-small')\n",
    "\n",
    "abs_path = os.path.abspath('')\n",
    "dir = os.path.dirname(abs_path)\n",
    "\n",
    "plt.savefig(abs_path + \"\\\\attempt_2_plot.pdf\")\n",
    "print(f\"Finished in {(time.time() - time_start)} seconds.\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "367639894a33c91e626d8a9fbfef352d67e646e16d9306bc55cfb030e5a73a17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
